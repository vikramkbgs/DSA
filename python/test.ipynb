{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset success: 10\n",
      "offset success: 20\n",
      "offset success: 30\n",
      "offset success: 40\n",
      "offset success: 50\n",
      "offset success: 60\n",
      "offset success: 70\n",
      "offset success: 80\n",
      "offset success: 90\n",
      "offset success: 100\n",
      "offset success: 110\n",
      "offset success: 120\n",
      "offset success: 130\n",
      "offset success: 140\n",
      "offset success: 150\n",
      "offset success: 160\n",
      "offset success: 170\n",
      "offset success: 180\n",
      "offset success: 190\n",
      "offset success: 200\n",
      "offset success: 210\n",
      "offset success: 220\n",
      "offset success: 230\n",
      "offset success: 240\n",
      "offset success: 250\n",
      "offset success: 260\n",
      "offset success: 270\n",
      "offset success: 280\n",
      "offset success: 290\n",
      "offset success: 300\n",
      "offset success: 310\n",
      "offset success: 320\n",
      "offset success: 330\n",
      "offset success: 340\n",
      "offset success: 350\n",
      "offset success: 360\n",
      "offset success: 370\n",
      "offset success: 380\n",
      "offset success: 390\n",
      "offset success: 400\n",
      "offset success: 410\n",
      "offset success: 420\n",
      "offset success: 430\n",
      "offset success: 440\n",
      "offset success: 450\n",
      "offset success: 460\n",
      "offset success: 470\n",
      "offset success: 480\n",
      "offset success: 490\n",
      "offset success: 500\n",
      "offset success: 510\n",
      "offset success: 520\n",
      "offset success: 530\n",
      "offset success: 540\n",
      "offset success: 550\n",
      "offset success: 560\n",
      "offset success: 570\n",
      "offset success: 580\n",
      "offset success: 590\n",
      "offset success: 600\n",
      "offset success: 610\n",
      "offset success: 620\n",
      "offset success: 630\n",
      "offset success: 640\n",
      "offset success: 650\n",
      "offset success: 660\n",
      "offset success: 670\n",
      "offset success: 680\n",
      "offset success: 690\n",
      "offset success: 700\n",
      "offset success: 710\n",
      "offset success: 720\n",
      "offset success: 730\n",
      "offset success: 740\n",
      "offset success: 750\n",
      "offset success: 760\n",
      "offset success: 770\n",
      "offset success: 780\n",
      "offset success: 790\n",
      "offset success: 800\n",
      "offset success: 810\n",
      "offset success: 820\n",
      "offset success: 830\n",
      "offset success: 840\n",
      "offset success: 850\n",
      "offset success: 860\n",
      "offset success: 870\n",
      "offset success: 880\n",
      "offset success: 890\n",
      "offset success: 900\n",
      "Total objects fetched: 900\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "offset_val = 0\n",
    "\n",
    "# Define the base URL without filter parameters\n",
    "base_url = \"https://data.exactspace.co/exactapi/units/60ae9143e284d016d3559dfb/activities\"\n",
    "\n",
    "all_objects = []  # List to store all fetched objects\n",
    "\n",
    "while len(all_objects) < 900:  # Continue fetching until desired number is reached\n",
    "    \n",
    "    # Construct the URL with the updated offset value\n",
    "    url = f\"{base_url}?filter=%7B%20%20%20%20%20%22where%22%3A%20%7B%20%20%20%20%20%20%20%20%20%22type%22%3A%20%22task%22%20%20%7D%2C%20%20%20%20%20%22order%22%3A%20%22createdOn%20DESC%22%2C%20%20%20%20%20%22limit%22%3A%2010%2C%20%22offset%22%3A{offset_val}%20%7D&access_token=Ziv35SH8DYvWbh0ZL8HMhOCESfQTwajS7j8iHfb1A3pEoSO6P8NraCLmCD1g7iAH\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        objects_fetched = len(data)  # Number of objects fetched in this request\n",
    "        all_objects.extend(data)      # Add fetched objects to the list\n",
    "        offset_val += 10\n",
    "        print(\"offset success:\", offset_val)\n",
    "    else:\n",
    "        print(\"Failed to fetch data. Status code:\", response.status_code)\n",
    "        break\n",
    "\n",
    "print(\"Total objects fetched:\", len(all_objects))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'task',\n",
       " 'voteAcceptCount': 0,\n",
       " 'voteRejectCount': 0,\n",
       " 'acceptedUserList': [],\n",
       " 'rejectedUserList': [],\n",
       " 'dueDate': '2024-04-23T06:38:57.962Z',\n",
       " 'assignee': None,\n",
       " 'source': 'PM',\n",
       " 'team': 'Maintenance',\n",
       " 'createdBy': None,\n",
       " 'createdOn': '2024-04-22T06:38:40.000Z',\n",
       " 'siteId': '65817c061be4c00007dc7f02',\n",
       " 'subTaskOf': None,\n",
       " 'subTasks': [],\n",
       " 'chats': [],\n",
       " 'chatOf': None,\n",
       " 'taskPriority': None,\n",
       " 'updateHistory': [{'action': 'Pulse created this task',\n",
       "   'on': '2024-04-22T06:38:40.000Z',\n",
       "   'by': ''}],\n",
       " 'id': '6625b8aa3f0e4100065ba5f9',\n",
       " 'unitsId': '65817c16dcd5de0007bbbf5d',\n",
       " 'status': 'new',\n",
       " 'sourceURL': '/rotary-assets/equipment/658192b61be4c00007dc8136?unitId=65817c16dcd5de0007bbbf5d&pm=660fbf49e009d9000790f00b',\n",
       " 'pmId': '660fbf49e009d9000790f00b',\n",
       " 'taskGeneratedBy': 'system',\n",
       " 'recur': True,\n",
       " 'content': [{'type': 'title', 'value': 'Test 2'}]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_objects[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to find or create worksheet based on title\n",
    "def find_or_create_worksheet(workbook, title):\n",
    "    for sheet in workbook.worksheets():\n",
    "        if sheet.get_name().lower() == title.lower():\n",
    "            return sheet\n",
    "    return workbook.add_worksheet(title)\n",
    "\n",
    "# Function to extract title, subtitles, and table data from content\n",
    "def extract_data(content):\n",
    "    title = \"\"\n",
    "    subtitles = []\n",
    "    tables = []\n",
    "\n",
    "    for item in content:\n",
    "        if item[\"type\"] == \"title\":\n",
    "            title = item[\"value\"]\n",
    "        elif item[\"type\"] == \"text\":\n",
    "            subtitles.append(item[\"value\"])\n",
    "        elif item[\"type\"] == \"table\":\n",
    "            tables.append(item[\"value\"])\n",
    "\n",
    "    return title, subtitles, tables\n",
    "\n",
    "# Function to write data to Excel\n",
    "def write_to_excel(title, subtitles, tables, update_history, workbook):\n",
    "    if \"GAP: GSD Recommendations\" in title:\n",
    "        worksheet = find_or_create_worksheet(workbook, \"Recommendation\")\n",
    "    elif \"GAP Shift\" in title:\n",
    "        worksheet = find_or_create_worksheet(workbook, \"Shift Report\")\n",
    "    else:\n",
    "        return  # Skip writing for other titles\n",
    "\n",
    "    start_row = worksheet.dim_rowmax or 0  # Get the last used row, or 0 if the sheet is empty\n",
    "\n",
    "    # Title formatting\n",
    "    title_format = workbook.add_format({'bold': True, 'font_color': 'white', 'bg_color': '#0070C0', 'align': 'center', 'valign': 'vcenter', 'font_size': 14})\n",
    "    worksheet.merge_range(f'A{start_row + 1}:F{start_row + 1}', title, title_format)  # Adjusted to start from next row\n",
    "    start_row += 2  # Move to the next row after the title\n",
    "\n",
    "    # Subtitles formatting\n",
    "    subtitle_format = workbook.add_format({'bold': True, 'font_color': 'black', 'bg_color': '#E0E0E0'})\n",
    "    for subtitle in subtitles:\n",
    "        worksheet.write(start_row, 0, subtitle, subtitle_format)\n",
    "        start_row += 1\n",
    "\n",
    "    # Write tables\n",
    "    start_row += 1  # Adding some empty rows between subtitles and tables\n",
    "    for table_data in tables:\n",
    "        # Set column width\n",
    "        worksheet.set_column('A:F', 33)\n",
    "        # Bold and light background color for the first row\n",
    "        bold_format = workbook.add_format({'bold': True, 'bg_color': '#E0E0E0'})\n",
    "        for j, value in enumerate(table_data[0]):\n",
    "            worksheet.write(start_row, j, value, bold_format)\n",
    "        start_row += 1\n",
    "        for row_data in table_data[1:]:\n",
    "            for j, value in enumerate(row_data):\n",
    "                worksheet.write(start_row, j, value)\n",
    "            start_row += 1\n",
    "        start_row += 2  # Adding some empty rows between tables\n",
    "\n",
    "    # Additional table for updateHistory\n",
    "    update_history_data = [\n",
    "        [\"Action\", \"Time\"],\n",
    "        *[(history[\"action\"], history[\"on\"]) for history in update_history]\n",
    "    ]\n",
    "    bold_format = workbook.add_format({'bold': True})\n",
    "    worksheet.write(start_row, 0, \"Action Taken in task\", bold_format)\n",
    "    start_row += 1\n",
    "    for row_data in update_history_data:\n",
    "        for j, value in enumerate(row_data):\n",
    "            worksheet.write(start_row, j, value)\n",
    "        start_row += 1\n",
    "\n",
    "# Function to write status counts to the \"Statistics\" sheet\n",
    "def write_status_counts_to_excel(all_objects, workbook):\n",
    "    # Initialize status count dictionary\n",
    "    status_count = {\"--\": 0, \"inprogress\": 0, \"deferred\": 0, \"notstarted\": 0, \"done\": 0}\n",
    "\n",
    "    # Count statuses\n",
    "    for obj in all_objects:\n",
    "        status = obj.get(\"status\", \"--\")\n",
    "        status_count[status] += 1\n",
    "\n",
    "    # Create or find the \"Statistics\" sheet\n",
    "    statistics_sheet = workbook.add_worksheet(\"Statistics\")\n",
    "\n",
    "    # Define bold format\n",
    "    bold_format = workbook.add_format({'bold': True})\n",
    "\n",
    "    # Write headers\n",
    "    statistics_sheet.write('A1', 'Status', bold_format)\n",
    "    statistics_sheet.write('B1', 'Count', bold_format)\n",
    "\n",
    "    # Write status counts\n",
    "    for idx, (status, count) in enumerate(status_count.items(), start=2):\n",
    "        statistics_sheet.write(f'A{idx}', status)\n",
    "        statistics_sheet.write(f'B{idx}', count)\n",
    "\n",
    "# Extract data from JSON and write to Excel\n",
    "def extract_and_write_to_excel(all_objects):\n",
    "    with pd.ExcelWriter('reports.xlsx', engine='xlsxwriter') as writer:\n",
    "        workbook = writer.book\n",
    "\n",
    "        for obj in all_objects:\n",
    "            title, subtitles, tables = extract_data(obj[\"content\"])\n",
    "            update_history = obj.get(\"updateHistory\", [])  # Get updateHistory data if available\n",
    "            write_to_excel(title, subtitles, tables, update_history, workbook)\n",
    "\n",
    "        # Write status counts to the \"Statistics\" sheet\n",
    "        write_status_counts_to_excel(all_objects, workbook)\n",
    "\n",
    "    print(\"Excel file saved successfully.\")\n",
    "\n",
    "extract_and_write_to_excel(all_objects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to find or create worksheet based on title\n",
    "def find_or_create_worksheet(workbook, title):\n",
    "    for sheet in workbook.worksheets():\n",
    "        if sheet.get_name().lower() == title.lower():\n",
    "            return sheet\n",
    "    return workbook.add_worksheet(title)\n",
    "\n",
    "# Function to extract title, subtitles, and table data from content\n",
    "def extract_data(content):\n",
    "    title = \"\"\n",
    "    subtitles = []\n",
    "    tables = []\n",
    "\n",
    "    for item in content:\n",
    "        if item[\"type\"] == \"title\":\n",
    "            title = item[\"value\"]\n",
    "        elif item[\"type\"] == \"text\":\n",
    "            subtitles.append(item[\"value\"])\n",
    "        elif item[\"type\"] == \"table\":\n",
    "            tables.append(item[\"value\"])\n",
    "\n",
    "    return title, subtitles, tables\n",
    "\n",
    "# Function to write data to Excel\n",
    "def write_to_excel(title, subtitles, tables, update_history, workbook):\n",
    "    if \"GAP: GSD Recommendations\" in title:\n",
    "        worksheet = find_or_create_worksheet(workbook, \"Recommendation\")\n",
    "    elif \"GAP Shift\" in title:\n",
    "        worksheet = find_or_create_worksheet(workbook, \"Shift Report\")\n",
    "    else:\n",
    "        return  # Skip writing for other titles\n",
    "\n",
    "    start_row = worksheet.dim_rowmax or 0  # Get the last used row, or 0 if the sheet is empty\n",
    "\n",
    "    # Title formatting\n",
    "    title_format = workbook.add_format({'bold': True, 'font_color': 'white', 'bg_color': '#0070C0', 'align': 'center', 'valign': 'vcenter', 'font_size': 14})\n",
    "    worksheet.merge_range(f'A{start_row + 1}:F{start_row + 1}', title, title_format)  # Adjusted to start from next row\n",
    "    start_row += 2  # Move to the next row after the title\n",
    "\n",
    "    # Subtitles formatting\n",
    "    subtitle_format = workbook.add_format({'bold': True, 'font_color': 'black', 'bg_color': '#E0E0E0'})\n",
    "    for subtitle in subtitles:\n",
    "        worksheet.write(start_row, 0, subtitle, subtitle_format)\n",
    "        start_row += 1\n",
    "\n",
    "    # Write tables\n",
    "    start_row += 1  # Adding some empty rows between subtitles and tables\n",
    "    for table_data in tables:\n",
    "        # Set column width\n",
    "        worksheet.set_column('A:F', 33)\n",
    "        # Bold and light background color for the first row\n",
    "        bold_format = workbook.add_format({'bold': True, 'bg_color': '#E0E0E0'})\n",
    "        for j, value in enumerate(table_data[0]):\n",
    "            worksheet.write(start_row, j, value, bold_format)\n",
    "        start_row += 1\n",
    "        for row_data in table_data[1:]:\n",
    "            for j, value in enumerate(row_data):\n",
    "                worksheet.write(start_row, j, value)\n",
    "            start_row += 1\n",
    "        start_row += 2  # Adding some empty rows between tables\n",
    "\n",
    "    # Additional table for updateHistory\n",
    "    update_history_data = [\n",
    "        [\"Action\", \"Time\"],\n",
    "        *[(history[\"action\"], history[\"on\"]) for history in update_history]\n",
    "    ]\n",
    "    bold_format = workbook.add_format({'bold': True})\n",
    "    worksheet.write(start_row, 0, \"Action Taken in task\", bold_format)\n",
    "    start_row += 1\n",
    "    for row_data in update_history_data:\n",
    "        for j, value in enumerate(row_data):\n",
    "            worksheet.write(start_row, j, value)\n",
    "        start_row += 1\n",
    "\n",
    "# Function to write status counts to the \"Statistics\" sheet\n",
    "def write_status_counts_to_excel(all_objects, workbook):\n",
    "    # Initialize status count dictionaries for Recommendation and Shift Report\n",
    "    status_count_recommendation = {\"--\": 0, \"inprogress\": 0, \"deferred\": 0, \"notstarted\": 0, \"done\": 0}\n",
    "    status_count_shift_report = {\"--\": 0, \"inprogress\": 0, \"deferred\": 0, \"notstarted\": 0, \"done\": 0}\n",
    "\n",
    "    # Count statuses\n",
    "    for obj in all_objects:\n",
    "        status = obj.get(\"status\", \"--\")\n",
    "        title = obj.get(\"content\", [{}])[0].get(\"value\", \"\")\n",
    "\n",
    "        if \"GAP: GSD Recommendations\" in title:\n",
    "            status_count_recommendation[status] += 1\n",
    "        elif \"GAP Shift\" in title:\n",
    "            status_count_shift_report[status] += 1\n",
    "\n",
    "    # Create or find the \"Statistics\" sheet\n",
    "    statistics_sheet = workbook.add_worksheet(\"Statistics\")\n",
    "\n",
    "    # Define bold format\n",
    "    bold_format = workbook.add_format({'bold': True})\n",
    "\n",
    "    # Write headers\n",
    "    statistics_sheet.write('A1', 'Status', bold_format)\n",
    "    statistics_sheet.write('B1', 'Count (Recommendation)', bold_format)\n",
    "    statistics_sheet.write('C1', 'Count (Shift Report)', bold_format)\n",
    "\n",
    "    # Write status counts for Recommendation\n",
    "    for idx, (status, count) in enumerate(status_count_recommendation.items(), start=2):\n",
    "        statistics_sheet.write(f'A{idx}', status)\n",
    "        statistics_sheet.write(f'B{idx}', count)\n",
    "\n",
    "    # Write status counts for Shift Report\n",
    "    for idx, (status, count) in enumerate(status_count_shift_report.items(), start=2):\n",
    "        statistics_sheet.write(f'C{idx}', count)\n",
    "\n",
    "# Extract data from JSON and write to Excel\n",
    "def extract_and_write_to_excel(all_objects):\n",
    "    with pd.ExcelWriter('reports.xlsx', engine='xlsxwriter') as writer:\n",
    "        workbook = writer.book\n",
    "\n",
    "        for obj in all_objects:\n",
    "            title, subtitles, tables = extract_data(obj.get(\"content\", []))\n",
    "            update_history = obj.get(\"updateHistory\", [])  # Get updateHistory data if available\n",
    "            write_to_excel(title, subtitles, tables, update_history, workbook)\n",
    "\n",
    "        # Write status counts to the \"Statistics\" sheet\n",
    "        write_status_counts_to_excel(all_objects, workbook)\n",
    "\n",
    "    print(\"Excel file saved successfully.\")\n",
    "\n",
    "extract_and_write_to_excel(all_objects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to find or create worksheet based on title\n",
    "def find_or_create_worksheet(workbook, title):\n",
    "    for sheet in workbook.worksheets():\n",
    "        if sheet.get_name().lower() == title.lower():\n",
    "            return sheet\n",
    "    return workbook.add_worksheet(title)\n",
    "\n",
    "# Function to extract title, subtitles, and table data from content\n",
    "def extract_data(content):\n",
    "    title = \"\"\n",
    "    subtitles = []\n",
    "    tables = []\n",
    "\n",
    "    for item in content:\n",
    "        if item[\"type\"] == \"title\":\n",
    "            title = item[\"value\"]\n",
    "        elif item[\"type\"] == \"text\":\n",
    "            subtitles.append(item[\"value\"])\n",
    "        elif item[\"type\"] == \"table\":\n",
    "            tables.append(item[\"value\"])\n",
    "\n",
    "    return title, subtitles, tables\n",
    "\n",
    "# Function to write data to Excel\n",
    "def write_to_excel(title, subtitles, tables, update_history, workbook):\n",
    "    if \"GAP: GSD Recommendations\" in title:\n",
    "        worksheet = find_or_create_worksheet(workbook, \"Recommendation\")\n",
    "    elif \"GAP Shift\" in title:\n",
    "        worksheet = find_or_create_worksheet(workbook, \"Shift Report\")\n",
    "    else:\n",
    "        return  # Skip writing for other titles\n",
    "\n",
    "    start_row = worksheet.dim_rowmax or 0  # Get the last used row, or 0 if the sheet is empty\n",
    "\n",
    "    # Title formatting\n",
    "    title_format = workbook.add_format({'bold': True, 'font_color': 'white', 'bg_color': '#0070C0', 'align': 'center', 'valign': 'vcenter', 'font_size': 14})\n",
    "    worksheet.merge_range(f'A{start_row + 1}:F{start_row + 1}', title, title_format)  # Adjusted to start from next row\n",
    "    start_row += 2  # Move to the next row after the title\n",
    "\n",
    "    # Subtitles formatting\n",
    "    subtitle_format = workbook.add_format({'bold': True, 'font_color': 'black', 'bg_color': '#E0E0E0'})\n",
    "    for subtitle in subtitles:\n",
    "        worksheet.write(start_row, 0, subtitle, subtitle_format)\n",
    "        start_row += 1\n",
    "\n",
    "    # Write tables\n",
    "    start_row += 1  # Adding some empty rows between subtitles and tables\n",
    "    for table_data in tables:\n",
    "        # Set column width\n",
    "        worksheet.set_column('A:F', 33)\n",
    "        # Bold and light background color for the first row\n",
    "        bold_format = workbook.add_format({'bold': True, 'bg_color': '#E0E0E0'})\n",
    "        for j, value in enumerate(table_data[0]):\n",
    "            worksheet.write(start_row, j, value, bold_format)\n",
    "        start_row += 1\n",
    "        for row_data in table_data[1:]:\n",
    "            for j, value in enumerate(row_data):\n",
    "                worksheet.write(start_row, j, value)\n",
    "            start_row += 1\n",
    "        start_row += 2  # Adding some empty rows between tables\n",
    "\n",
    "    # Additional table for updateHistory\n",
    "    update_history_data = [\n",
    "        [\"Action\", \"Time\"],\n",
    "        *[(history[\"action\"], history[\"on\"]) for history in update_history]\n",
    "    ]\n",
    "    bold_format = workbook.add_format({'bold': True})\n",
    "    worksheet.write(start_row, 0, \"Action Taken in task\", bold_format)\n",
    "    start_row += 1\n",
    "    for row_data in update_history_data:\n",
    "        for j, value in enumerate(row_data):\n",
    "            worksheet.write(start_row, j, value)\n",
    "        start_row += 1\n",
    "\n",
    "# Function to write status counts to the \"Statistics\" sheet\n",
    "def write_status_counts_to_excel(all_objects, workbook):\n",
    "    # Initialize status count dictionaries for Recommendation and Shift Report\n",
    "    status_count_recommendation = {\"--\": 0, \"inprogress\": 0, \"deferred\": 0, \"notstarted\": 0, \"done\": 0}\n",
    "    update_history_count_recommendation = {\"Empty\": 0, \"One comment\": 0, \"More than one comment\": 0}\n",
    "    status_count_shift_report = {\"--\": 0, \"inprogress\": 0, \"deferred\": 0, \"notstarted\": 0, \"done\": 0}\n",
    "    update_history_count_shift_report = {\"Empty\": 0, \"One comment\": 0, \"More than one comment\": 0}\n",
    "\n",
    "    # Count statuses and updateHistory entries\n",
    "    for obj in all_objects:\n",
    "        status = obj.get(\"status\", \"--\")\n",
    "        title = obj.get(\"content\", [{}])[0].get(\"value\", \"\")\n",
    "        update_history = obj.get(\"updateHistory\", [])\n",
    "        num_comments = len(update_history)\n",
    "\n",
    "        if \"GAP: GSD Recommendations\" in title:\n",
    "            status_count_recommendation[status] += 1\n",
    "            if num_comments == 0:\n",
    "                update_history_count_recommendation[\"Empty\"] += 1\n",
    "            elif num_comments == 1:\n",
    "                update_history_count_recommendation[\"One comment\"] += 1\n",
    "            else:\n",
    "                update_history_count_recommendation[\"More than one comment\"] += 1\n",
    "        elif \"GAP Shift\" in title:\n",
    "            status_count_shift_report[status] += 1\n",
    "            if num_comments == 0:\n",
    "                update_history_count_shift_report[\"Empty\"] += 1\n",
    "            elif num_comments == 1:\n",
    "                update_history_count_shift_report[\"One comment\"] += 1\n",
    "            else:\n",
    "                update_history_count_shift_report[\"More than one comment\"] += 1\n",
    "\n",
    "    # Create or find the \"Statistics\" sheet\n",
    "    statistics_sheet = workbook.add_worksheet(\"Statistics\")\n",
    "\n",
    "    # Define bold format\n",
    "    bold_format = workbook.add_format({'bold': True})\n",
    "    statistics_sheet.set_column('A:Z', 25)\n",
    "\n",
    "    # Write headers for Recommendation\n",
    "    statistics_sheet.write('A1', 'Status(Recommendations)', bold_format)\n",
    "    statistics_sheet.write('B1', 'Count (Recommendations)', bold_format)\n",
    "    statistics_sheet.write('C1', 'Action(Recommendations)', bold_format)\n",
    "    statistics_sheet.write('D1', 'Count(Recommendations)', bold_format)\n",
    "\n",
    "    # Write status counts for Recommendation\n",
    "    for idx, (status, count) in enumerate(status_count_recommendation.items(), start=2):\n",
    "        statistics_sheet.write(f'A{idx}', status)\n",
    "        statistics_sheet.write(f'B{idx}', count)\n",
    "    for idx, (count_type, count) in enumerate(update_history_count_recommendation.items(), start=2):\n",
    "        statistics_sheet.write(f'C{idx}', count_type)\n",
    "        statistics_sheet.write(f'D{idx}', count)\n",
    "\n",
    "    # Write headers for Shift Report\n",
    "    statistics_sheet.write('F1', 'Status(Shift Report)', bold_format)\n",
    "    statistics_sheet.write('G1', 'Count(Shift Report)', bold_format)\n",
    "    statistics_sheet.write('H1', 'Action(Shift Report)', bold_format)\n",
    "    statistics_sheet.write('I1', 'Count(Shift Report)', bold_format)\n",
    "\n",
    "    # Write status counts for Shift Report\n",
    "    for idx, (status, count) in enumerate(status_count_shift_report.items(), start=2):\n",
    "        statistics_sheet.write(f'F{idx}', status)\n",
    "        statistics_sheet.write(f'G{idx}', count)\n",
    "    for idx, (count_type, count) in enumerate(update_history_count_shift_report.items(), start=2):\n",
    "        statistics_sheet.write(f'H{idx}', count_type)\n",
    "        statistics_sheet.write(f'I{idx}', count)\n",
    "\n",
    "# Extract data from JSON and write to Excel\n",
    "def extract_and_write_to_excel(all_objects):\n",
    "    with pd.ExcelWriter('reports.xlsx', engine='xlsxwriter') as writer:\n",
    "        workbook = writer.book\n",
    "\n",
    "        for obj in all_objects:\n",
    "            title, subtitles, tables = extract_data(obj.get(\"content\", []))\n",
    "            update_history = obj.get(\"updateHistory\", [])  # Get updateHistory data if available\n",
    "            write_to_excel(title, subtitles, tables, update_history, workbook)\n",
    "\n",
    "        # Write status counts to the \"Statistics\" sheet\n",
    "        write_status_counts_to_excel(all_objects, workbook)\n",
    "\n",
    "    print(\"Excel file saved successfully.\")\n",
    "\n",
    "extract_and_write_to_excel(all_objects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to find or create worksheet based on title\n",
    "def find_or_create_worksheet(workbook, title):\n",
    "    for sheet in workbook.worksheets():\n",
    "        if sheet.get_name().lower() == title.lower():\n",
    "            return sheet\n",
    "    return workbook.add_worksheet(title)\n",
    "\n",
    "# Function to extract title, subtitles, and table data from content\n",
    "def extract_data(content):\n",
    "    title = \"\"\n",
    "    subtitles = []\n",
    "    tables = []\n",
    "\n",
    "    for item in content:\n",
    "        if item[\"type\"] == \"title\":\n",
    "            title = item[\"value\"]\n",
    "        elif item[\"type\"] == \"text\":\n",
    "            subtitles.append(item[\"value\"])\n",
    "        elif item[\"type\"] == \"table\":\n",
    "            tables.append(item[\"value\"])\n",
    "\n",
    "    return title, subtitles, tables\n",
    "\n",
    "# Function to write data to Excel\n",
    "def write_to_excel(title, subtitles, tables, update_history, workbook):\n",
    "    if \"GAP: GSD Recommendations\" in title:\n",
    "        worksheet = find_or_create_worksheet(workbook, \"Recommendation\")\n",
    "    elif \"GAP Shift\" in title:\n",
    "        worksheet = find_or_create_worksheet(workbook, \"Shift Report\")\n",
    "    else:\n",
    "        return  # Skip writing for other titles\n",
    "\n",
    "    start_row = worksheet.dim_rowmax or 0  # Get the last used row, or 0 if the sheet is empty\n",
    "\n",
    "    # Title formatting\n",
    "    title_format = workbook.add_format({'bold': True, 'font_color': 'white', 'bg_color': '#0070C0', 'align': 'center', 'valign': 'vcenter', 'font_size': 14})\n",
    "    worksheet.merge_range(f'A{start_row + 1}:F{start_row + 1}', title, title_format)  # Adjusted to start from next row\n",
    "    start_row += 2  # Move to the next row after the title\n",
    "\n",
    "    # Subtitles formatting\n",
    "    subtitle_format = workbook.add_format({'bold': True, 'font_color': 'black', 'bg_color': '#E0E0E0'})\n",
    "    for subtitle in subtitles:\n",
    "        worksheet.write(start_row, 0, subtitle, subtitle_format)\n",
    "        start_row += 1\n",
    "\n",
    "    # If not update history, write tables\n",
    "    if update_history:\n",
    "        # Write tables\n",
    "        start_row += 1  # Adding some empty rows between subtitles and tables\n",
    "        for table_data in tables:\n",
    "            # Set column width\n",
    "            worksheet.set_column('A:F', 33)\n",
    "            # Bold and light background color for the first row\n",
    "            bold_format = workbook.add_format({'bold': True, 'bg_color': '#E0E0E0'})\n",
    "            for j, value in enumerate(table_data[0]):\n",
    "                worksheet.write(start_row, j, value, bold_format)\n",
    "            start_row += 1\n",
    "            for row_data in table_data[1:]:\n",
    "                for j, value in enumerate(row_data):\n",
    "                    worksheet.write(start_row, j, value)\n",
    "                start_row += 1\n",
    "            start_row += 2  # Adding some empty rows between tables\n",
    "\n",
    "    # Additional table for updateHistory\n",
    "    update_history_data = [\n",
    "        [\"Action\", \"Time\"],\n",
    "        *[(history[\"action\"], history[\"on\"]) for history in update_history]\n",
    "    ]\n",
    "    bold_format = workbook.add_format({'bold': True})\n",
    "    worksheet.write(start_row, 0, \"Action Taken in task\", bold_format)\n",
    "    start_row += 1\n",
    "    for row_data in update_history_data:\n",
    "        for j, value in enumerate(row_data):\n",
    "            worksheet.write(start_row, j, value)\n",
    "        start_row += 1\n",
    "\n",
    "# Function to write status counts to the \"Statistics\" sheet\n",
    "def write_status_counts_to_excel(all_objects, workbook):\n",
    "    # Initialize status count dictionaries for Recommendation and Shift Report\n",
    "    status_count_recommendation = {\"--\": 0, \"inprogress\": 0, \"deferred\": 0, \"notstarted\": 0, \"done\": 0}\n",
    "    update_history_count_recommendation = {\"Empty\": 0, \"One comment\": 0, \"More than one comment\": 0}\n",
    "    status_count_shift_report = {\"--\": 0, \"inprogress\": 0, \"deferred\": 0, \"notstarted\": 0, \"done\": 0}\n",
    "    update_history_count_shift_report = {\"Empty\": 0, \"One comment\": 0, \"More than one comment\": 0}\n",
    "\n",
    "    # Count statuses and updateHistory entries\n",
    "    for obj in all_objects:\n",
    "        status = obj.get(\"status\", \"--\")\n",
    "        title = obj.get(\"content\", [{}])[0].get(\"value\", \"\")\n",
    "        update_history = obj.get(\"updateHistory\", [])\n",
    "        num_comments = len(update_history)\n",
    "\n",
    "        if \"GAP: GSD Recommendations\" in title:\n",
    "            status_count_recommendation[status] += 1\n",
    "            if num_comments == 0:\n",
    "                update_history_count_recommendation[\"Empty\"] += 1\n",
    "            elif num_comments == 1:\n",
    "                update_history_count_recommendation[\"One comment\"] += 1\n",
    "            else:\n",
    "                update_history_count_recommendation[\"More than one comment\"] += 1\n",
    "        elif \"GAP Shift\" in title:\n",
    "            status_count_shift_report[status] += 1\n",
    "            if num_comments == 0:\n",
    "                update_history_count_shift_report[\"Empty\"] += 1\n",
    "            elif num_comments == 1:\n",
    "                update_history_count_shift_report[\"One comment\"] += 1\n",
    "            else:\n",
    "                update_history_count_shift_report[\"More than one comment\"] += 1\n",
    "\n",
    "    # Create or find the \"Statistics\" sheet\n",
    "    statistics_sheet = workbook.add_worksheet(\"Statistics\")\n",
    "\n",
    "    # Define bold format\n",
    "    bold_format = workbook.add_format({'bold': True})\n",
    "    statistics_sheet.set_column('A:I', 18)\n",
    "\n",
    "    # Write headers for Recommendation\n",
    "    statistics_sheet.write('A1', 'Status(Recommendations)', bold_format)\n",
    "    statistics_sheet.write('B1', 'Count (Recommendations)', bold_format)\n",
    "    statistics_sheet.write('C1', 'Action(Recommendations)', bold_format)\n",
    "    statistics_sheet.write('D1', 'Count(Recommendations)', bold_format)\n",
    "\n",
    "    # Write status counts for Recommendation\n",
    "    for idx, (status, count) in enumerate(status_count_recommendation.items(), start=2):\n",
    "        statistics_sheet.write(f'A{idx}', status)\n",
    "        statistics_sheet.write(f'B{idx}', count)\n",
    "    for idx, (count_type, count) in enumerate(update_history_count_recommendation.items(), start=2):\n",
    "        statistics_sheet.write(f'C{idx}', count_type)\n",
    "        statistics_sheet.write(f'D{idx}', count)\n",
    "\n",
    "    # Write headers for Shift Report\n",
    "    statistics_sheet.write('F1', 'Status(Shift Report)', bold_format)\n",
    "    statistics_sheet.write('G1', 'Count(Shift Report)', bold_format)\n",
    "    statistics_sheet.write('H1', 'Action(Shift Report)', bold_format)\n",
    "    statistics_sheet.write('I1', 'Count(Shift Report)', bold_format)\n",
    "\n",
    "    # Write status counts for Shift Report\n",
    "    for idx, (status, count) in enumerate(status_count_shift_report.items(), start=2):\n",
    "        statistics_sheet.write(f'F{idx}', status)\n",
    "        statistics_sheet.write(f'G{idx}', count)\n",
    "    for idx, (count_type, count) in enumerate(update_history_count_shift_report.items(), start=2):\n",
    "        statistics_sheet.write(f'H{idx}', count_type)\n",
    "        statistics_sheet.write(f'I{idx}', count)\n",
    "\n",
    "# Extract data from JSON and write to Excel\n",
    "def extract_and_write_to_excel(all_objects):\n",
    "    with pd.ExcelWriter('reports.xlsx', engine='xlsxwriter') as writer:\n",
    "        workbook = writer.book\n",
    "\n",
    "        for obj in all_objects:\n",
    "            title, subtitles, tables = extract_data(obj.get(\"content\", []))\n",
    "            update_history = obj.get(\"updateHistory\", [])  # Get updateHistory data if available\n",
    "            write_to_excel(title, subtitles, tables, update_history, workbook)\n",
    "\n",
    "        # Write status counts to the \"Statistics\" sheet\n",
    "        write_status_counts_to_excel(all_objects, workbook)\n",
    "\n",
    "    print(\"Excel file saved successfully.\")\n",
    "\n",
    "extract_and_write_to_excel(all_objects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to find or create worksheet based on title\n",
    "def find_or_create_worksheet(workbook, title):\n",
    "    for sheet in workbook.worksheets():\n",
    "        if sheet.get_name().lower() == title.lower():\n",
    "            return sheet\n",
    "    return workbook.add_worksheet(title)\n",
    "\n",
    "# Function to extract title, subtitles, and table data from content\n",
    "def extract_data(content):\n",
    "    title = \"\"\n",
    "    subtitles = []\n",
    "    tables = []\n",
    "\n",
    "    for item in content:\n",
    "        if item[\"type\"] == \"title\":\n",
    "            title = item[\"value\"]\n",
    "        elif item[\"type\"] == \"text\":\n",
    "            subtitles.append(item[\"value\"])\n",
    "        elif item[\"type\"] == \"table\":\n",
    "            tables.append(item[\"value\"])\n",
    "\n",
    "    return title, subtitles, tables\n",
    "\n",
    "# Function to write data to Excel\n",
    "def write_to_excel(title, subtitles, tables, update_history, workbook):\n",
    "    if \"GAP: GSD Recommendations\" in title:\n",
    "        worksheet = find_or_create_worksheet(workbook, \"Recommendation\")\n",
    "    elif \"GAP Shift\" in title:\n",
    "        worksheet = find_or_create_worksheet(workbook, \"Shift Report\")\n",
    "    else:\n",
    "        return  # Skip writing for other titles\n",
    "\n",
    "    start_row = worksheet.dim_rowmax or 0  # Get the last used row, or 0 if the sheet is empty\n",
    "\n",
    "    # Title formatting\n",
    "    title_format = workbook.add_format({'bold': True, 'font_color': 'white', 'bg_color': '#0070C0', 'align': 'center', 'valign': 'vcenter', 'font_size': 14})\n",
    "    worksheet.merge_range(f'A{start_row + 1}:F{start_row + 1}', title, title_format)  # Adjusted to start from next row\n",
    "    start_row += 2  # Move to the next row after the title\n",
    "\n",
    "    # Subtitles formatting\n",
    "    subtitle_format = workbook.add_format({'bold': True, 'font_color': 'black', 'bg_color': '#E0E0E0'})\n",
    "    for subtitle in subtitles:\n",
    "        worksheet.write(start_row, 0, subtitle, subtitle_format)\n",
    "        start_row += 1\n",
    "\n",
    "    # If not update history, write tables\n",
    "    if update_history:\n",
    "        # Write tables\n",
    "        start_row += 1  # Adding some empty rows between subtitles and tables\n",
    "        for table_data in tables:\n",
    "            # Set column width\n",
    "            worksheet.set_column('A:F', 33)\n",
    "            # Bold and light background color for the first row\n",
    "            bold_format = workbook.add_format({'bold': True, 'bg_color': '#E0E0E0'})\n",
    "            for j, value in enumerate(table_data[0]):\n",
    "                worksheet.write(start_row, j, value, bold_format)\n",
    "            start_row += 1\n",
    "            for row_data in table_data[1:]:\n",
    "                for j, value in enumerate(row_data):\n",
    "                    worksheet.write(start_row, j, value)\n",
    "                start_row += 1\n",
    "            start_row += 2  # Adding some empty rows between tables\n",
    "\n",
    "    # Additional table for updateHistory\n",
    "    update_history_data = [\n",
    "        [\"Action\", \"Time\"],\n",
    "        *[(history[\"action\"], history[\"on\"]) for history in update_history]\n",
    "    ]\n",
    "    bold_format = workbook.add_format({'bold': True})\n",
    "    worksheet.write(start_row, 0, \"Action Taken in task\", bold_format)\n",
    "    start_row += 1\n",
    "    for row_data in update_history_data:\n",
    "        for j, value in enumerate(row_data):\n",
    "            worksheet.write(start_row, j, value)\n",
    "        start_row += 1\n",
    "\n",
    "# Function to write status counts to the \"Statistics\" sheet\n",
    "def write_status_counts_to_excel(all_objects, workbook):\n",
    "    # Initialize status count dictionaries for Recommendation and Shift Report\n",
    "    status_count_recommendation = {\"--\": 0, \"inprogress\": 0, \"deferred\": 0, \"notstarted\": 0, \"done\": 0}\n",
    "    update_history_count_recommendation = {\"Empty\": 0, \"One comment\": 0, \"More than one comment\": 0}\n",
    "    status_count_shift_report = {\"--\": 0, \"inprogress\": 0, \"deferred\": 0, \"notstarted\": 0, \"done\": 0}\n",
    "    update_history_count_shift_report = {\"Empty\": 0, \"One comment\": 0, \"More than one comment\": 0}\n",
    "\n",
    "    # Get the interval between the first and last object's createdOn times\n",
    "    first_created_time = None\n",
    "    last_created_time = None\n",
    "    for obj in all_objects:\n",
    "        created_time = datetime.strptime(obj.get(\"createdOn\", \"\"), \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "        if first_created_time is None or created_time < first_created_time:\n",
    "            first_created_time = created_time\n",
    "        if last_created_time is None or created_time > last_created_time:\n",
    "            last_created_time = created_time\n",
    "    \n",
    "    interval = (last_created_time - first_created_time).days  # Interval in days\n",
    "\n",
    "    # Count statuses and updateHistory entries\n",
    "    for obj in all_objects:\n",
    "        status = obj.get(\"status\", \"--\")\n",
    "        title = obj.get(\"content\", [{}])[0].get(\"value\", \"\")\n",
    "        update_history = obj.get(\"updateHistory\", [])\n",
    "        num_comments = len(update_history)\n",
    "\n",
    "        if \"GAP: GSD Recommendations\" in title:\n",
    "            status_count_recommendation[status] += 1\n",
    "            if num_comments == 0:\n",
    "                update_history_count_recommendation[\"Empty\"] += 1\n",
    "            elif num_comments == 1:\n",
    "                update_history_count_recommendation[\"One comment\"] += 1\n",
    "            else:\n",
    "                update_history_count_recommendation[\"More than one comment\"] += 1\n",
    "        elif \"GAP Shift\" in title:\n",
    "            status_count_shift_report[status] += 1\n",
    "            if num_comments == 0:\n",
    "                update_history_count_shift_report[\"Empty\"] += 1\n",
    "            elif num_comments == 1:\n",
    "                update_history_count_shift_report[\"One comment\"] += 1\n",
    "            else:\n",
    "                update_history_count_shift_report[\"More than one comment\"] += 1\n",
    "\n",
    "    # Create or find the \"Statistics\" sheet\n",
    "    statistics_sheet = workbook.add_worksheet(\"Statistics\")\n",
    "\n",
    "    # Define bold format\n",
    "    bold_format = workbook.add_format({'bold': True})\n",
    "    statistics_sheet.set_column('A:G', 25)\n",
    "\n",
    "    # Write title and interval\n",
    "    statistics_sheet.write('B1:E1', 'Statistics', bold_format)\n",
    "    statistics_sheet.write('A3', 'First Created On', bold_format)\n",
    "    statistics_sheet.write('B3', 'Last Created On', bold_format)\n",
    "    statistics_sheet.write('C3', 'Interval (Days)', bold_format)\n",
    "\n",
    "    statistics_sheet.write('A4', first_created_time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    statistics_sheet.write('B4', last_created_time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    statistics_sheet.write('C4', interval)\n",
    "\n",
    "    # Write headers for Recommendation\n",
    "    statistics_sheet.write('A7', 'Status(Recommendations)', bold_format)\n",
    "    statistics_sheet.write('B7', 'Count (Recommendations)', bold_format)\n",
    "    statistics_sheet.write('C7', 'Action(Recommendations)', bold_format)\n",
    "    statistics_sheet.write('D7', 'Count(Recommendations)', bold_format)\n",
    "\n",
    "    # Write status counts for Recommendation\n",
    "    for idx, (status, count) in enumerate(status_count_recommendation.items(), start=8):\n",
    "        statistics_sheet.write(f'A{idx}', status)\n",
    "        statistics_sheet.write(f'B{idx}', count)\n",
    "    for idx, (count_type, count) in enumerate(update_history_count_recommendation.items(), start=8):\n",
    "        statistics_sheet.write(f'C{idx}', count_type)\n",
    "        statistics_sheet.write(f'D{idx}', count)\n",
    "\n",
    "    # Write headers for Shift Report\n",
    "    statistics_sheet.write('A15', 'Status(Shift Report)', bold_format)\n",
    "    statistics_sheet.write('B15', 'Count(Shift Report)', bold_format)\n",
    "    statistics_sheet.write('C15', 'Action(Shift Report)', bold_format)\n",
    "    statistics_sheet.write('D15', 'Count(Shift Report)', bold_format)\n",
    "\n",
    "    # Write status counts for Shift Report\n",
    "    for idx, (status, count) in enumerate(status_count_shift_report.items(), start=16):\n",
    "        statistics_sheet.write(f'A{idx}', status)\n",
    "        statistics_sheet.write(f'B{idx}', count)\n",
    "    for idx, (count_type, count) in enumerate(update_history_count_shift_report.items(), start=16):\n",
    "        statistics_sheet.write(f'C{idx}', count_type)\n",
    "        statistics_sheet.write(f'D{idx}', count)\n",
    "\n",
    "# Extract data from JSON and write to Excel\n",
    "def extract_and_write_to_excel(all_objects):\n",
    "    with pd.ExcelWriter('reports.xlsx', engine='xlsxwriter') as writer:\n",
    "        workbook = writer.book\n",
    "\n",
    "        for obj in all_objects:\n",
    "            title, subtitles, tables = extract_data(obj.get(\"content\", []))\n",
    "            update_history = obj.get(\"updateHistory\", [])  # Get updateHistory data if available\n",
    "            write_to_excel(title, subtitles, tables, update_history, workbook)\n",
    "\n",
    "        # Write status counts to the \"Statistics\" sheet\n",
    "        write_status_counts_to_excel(all_objects, workbook)\n",
    "\n",
    "    print(\"Excel file saved successfully.\")\n",
    "\n",
    "extract_and_write_to_excel(all_objects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to find or create worksheet based on title\n",
    "def find_or_create_worksheet(workbook, title):\n",
    "    for sheet in workbook.worksheets():\n",
    "        if sheet.get_name().lower() == title.lower():\n",
    "            return sheet\n",
    "    return workbook.add_worksheet(title)\n",
    "\n",
    "# Function to extract title, subtitles, and table data from content\n",
    "def extract_data(content):\n",
    "    title = \"\"\n",
    "    subtitles = []\n",
    "    tables = []\n",
    "\n",
    "    for item in content:\n",
    "        if item[\"type\"] == \"title\":\n",
    "            title = item[\"value\"]\n",
    "        elif item[\"type\"] == \"text\":\n",
    "            subtitles.append(item[\"value\"])\n",
    "        elif item[\"type\"] == \"table\":\n",
    "            tables.append(item[\"value\"])\n",
    "\n",
    "    return title, subtitles, tables\n",
    "\n",
    "# Function to write data to Excel\n",
    "def write_to_excel(title, subtitles, tables, update_history, workbook):\n",
    "    if \"GAP: GSD Recommendations\" in title:\n",
    "        worksheet = find_or_create_worksheet(workbook, \"Recommendation\")\n",
    "    elif \"GAP Shift\" in title:\n",
    "        worksheet = find_or_create_worksheet(workbook, \"Shift Report\")\n",
    "    else:\n",
    "        return  # Skip writing for other titles\n",
    "\n",
    "    start_row = worksheet.dim_rowmax or 0  # Get the last used row, or 0 if the sheet is empty\n",
    "\n",
    "    # Title formatting\n",
    "    title_format = workbook.add_format({'bold': True, 'font_color': 'white', 'bg_color': '#0070C0', 'align': 'center', 'valign': 'vcenter', 'font_size': 14})\n",
    "    worksheet.merge_range(f'A{start_row + 1}:F{start_row + 1}', title, title_format)  # Adjusted to start from next row\n",
    "    start_row += 2  # Move to the next row after the title\n",
    "\n",
    "    # Subtitles formatting\n",
    "    subtitle_format = workbook.add_format({'bold': True, 'font_color': 'black', 'bg_color': '#E0E0E0'})\n",
    "    for subtitle in subtitles:\n",
    "        worksheet.write(start_row, 0, subtitle, subtitle_format)\n",
    "        start_row += 1\n",
    "\n",
    "    # If not update history, write tables\n",
    "    if update_history:\n",
    "        # Write tables\n",
    "        start_row += 1  # Adding some empty rows between subtitles and tables\n",
    "        for table_data in tables:\n",
    "            # Set column width\n",
    "            worksheet.set_column('A:F', 33)\n",
    "            # Bold and light background color for the first row\n",
    "            bold_format = workbook.add_format({'bold': True, 'bg_color': '#E0E0E0'})\n",
    "            for j, value in enumerate(table_data[0]):\n",
    "                worksheet.write(start_row, j, value, bold_format)\n",
    "            start_row += 1\n",
    "            for row_data in table_data[1:]:\n",
    "                for j, value in enumerate(row_data):\n",
    "                    worksheet.write(start_row, j, value)\n",
    "                start_row += 1\n",
    "            start_row += 2  # Adding some empty rows between tables\n",
    "\n",
    "    # Additional table for updateHistory\n",
    "    update_history_data = [\n",
    "        [\"Action\", \"Time\"],\n",
    "        *[(history[\"action\"], history[\"on\"]) for history in update_history]\n",
    "    ]\n",
    "    bold_format = workbook.add_format({'bold': True})\n",
    "    worksheet.write(start_row, 0, \"Action Taken in task\", bold_format)\n",
    "    start_row += 1\n",
    "    for row_data in update_history_data:\n",
    "        for j, value in enumerate(row_data):\n",
    "            worksheet.write(start_row, j, value)\n",
    "        start_row += 1\n",
    "\n",
    "# Function to write status counts to the \"Statistics\" sheet\n",
    "def write_status_counts_to_excel(all_objects, workbook):\n",
    "    # Initialize status count dictionaries for Recommendation and Shift Report\n",
    "    status_count_recommendation = {\"--\": 0, \"inprogress\": 0, \"deferred\": 0, \"notstarted\": 0, \"done\": 0}\n",
    "    update_history_count_recommendation = {\"Empty\": 0, \"One comment\": 0, \"More than one comment\": 0}\n",
    "    status_count_shift_report = {\"--\": 0, \"inprogress\": 0, \"deferred\": 0, \"notstarted\": 0, \"done\": 0}\n",
    "    update_history_count_shift_report = {\"Empty\": 0, \"One comment\": 0, \"More than one comment\": 0}\n",
    "\n",
    "    # Get the interval between the first and last object's createdOn times\n",
    "    first_created_time = None\n",
    "    last_created_time = None\n",
    "    for obj in all_objects:\n",
    "        created_time = datetime.strptime(obj.get(\"createdOn\", \"\"), \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "        if first_created_time is None or created_time < first_created_time:\n",
    "            first_created_time = created_time\n",
    "        if last_created_time is None or created_time > last_created_time:\n",
    "            last_created_time = created_time\n",
    "    \n",
    "    interval = (last_created_time - first_created_time).days  # Interval in days\n",
    "\n",
    "    # Count statuses and updateHistory entries\n",
    "    for obj in all_objects:\n",
    "        status = obj.get(\"status\", \"--\")\n",
    "        title = obj.get(\"content\", [{}])[0].get(\"value\", \"\")\n",
    "        update_history = obj.get(\"updateHistory\", [])\n",
    "        num_comments = len(update_history)\n",
    "\n",
    "        if \"GAP: GSD Recommendations\" in title:\n",
    "            status_count_recommendation[status] += 1\n",
    "            if num_comments == 0:\n",
    "                update_history_count_recommendation[\"Empty\"] += 1\n",
    "            elif num_comments == 1:\n",
    "                update_history_count_recommendation[\"One comment\"] += 1\n",
    "            else:\n",
    "                update_history_count_recommendation[\"More than one comment\"] += 1\n",
    "        elif \"GAP Shift\" in title:\n",
    "            status_count_shift_report[status] += 1\n",
    "            if num_comments == 0:\n",
    "                update_history_count_shift_report[\"Empty\"] += 1\n",
    "            elif num_comments == 1:\n",
    "                update_history_count_shift_report[\"One comment\"] += 1\n",
    "            else:\n",
    "                update_history_count_shift_report[\"More than one comment\"] += 1\n",
    "\n",
    "    # Create or find the \"Statistics\" sheet\n",
    "    statistics_sheet = workbook.add_worksheet(\"Statistics\")\n",
    "    statistics_sheet.set_column('A:Z', 18)\n",
    "\n",
    "    # Define bold format and cell formatting\n",
    "    bold_format = workbook.add_format({'bold': True})\n",
    "    header_format = workbook.add_format({'bold': True, 'bg_color': '#0070C0', 'font_color': 'white', 'align': 'center', 'valign': 'vcenter'})\n",
    "    data_format = workbook.add_format({'align': 'center', 'valign': 'vcenter'})\n",
    "    subtitle_format = workbook.add_format({'bold': True, 'bg_color': '#E0E0E0'})\n",
    "\n",
    "    # Write title and interval\n",
    "    statistics_sheet.merge_range('B1:E1', 'Statistics', header_format)\n",
    "    statistics_sheet.merge_range('A3:A4', 'Created On', subtitle_format)\n",
    "    statistics_sheet.merge_range('B3:B4', 'Last Created On', subtitle_format)\n",
    "    statistics_sheet.merge_range('C3:C4', 'Interval (Days)', subtitle_format)\n",
    "\n",
    "    statistics_sheet.write('A5', first_created_time.strftime(\"%Y-%m-%d %H:%M:%S\"), data_format)\n",
    "    statistics_sheet.write('B5', last_created_time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    statistics_sheet.write('C5', interval, data_format)\n",
    "\n",
    "    # Write headers for Recommendation\n",
    "    statistics_sheet.merge_range('A7:D7', 'Recommendations', header_format)\n",
    "    statistics_sheet.write('A8', 'Status', header_format)\n",
    "    statistics_sheet.write('B8', 'Count', header_format)\n",
    "    statistics_sheet.write('C8', 'Action', header_format)\n",
    "    statistics_sheet.write('D8', 'Count', header_format)\n",
    "\n",
    "    # Write status counts for Recommendation\n",
    "    for idx, (status, count) in enumerate(status_count_recommendation.items(), start=9):\n",
    "        statistics_sheet.write(f'A{idx}', status, data_format)\n",
    "        statistics_sheet.write(f'B{idx}', count, data_format)\n",
    "    for idx, (count_type, count) in enumerate(update_history_count_recommendation.items(), start=9):\n",
    "        statistics_sheet.write(f'C{idx}', count_type, data_format)\n",
    "        statistics_sheet.write(f'D{idx}', count, data_format)\n",
    "\n",
    "    # Write headers for Shift Report\n",
    "    statistics_sheet.merge_range('F7:I7', 'Shift Report', header_format)\n",
    "    statistics_sheet.write('F8', 'Status', header_format)\n",
    "    statistics_sheet.write('G8', 'Count', header_format)\n",
    "    statistics_sheet.write('H8', 'Action', header_format)\n",
    "    statistics_sheet.write('I8', 'Count', header_format)\n",
    "\n",
    "    # Write status counts for Shift Report\n",
    "    for idx, (status, count) in enumerate(status_count_shift_report.items(), start=9):\n",
    "        statistics_sheet.write(f'F{idx}', status, data_format)\n",
    "        statistics_sheet.write(f'G{idx}', count, data_format)\n",
    "    for idx, (count_type, count) in enumerate(update_history_count_shift_report.items(), start=9):\n",
    "        statistics_sheet.write(f'H{idx}', count_type, data_format)\n",
    "        statistics_sheet.write(f'I{idx}', count, data_format)\n",
    "\n",
    "# Extract data from JSON and write to Excel\n",
    "def extract_and_write_to_excel(all_objects):\n",
    "    with pd.ExcelWriter('reports_2204.xlsx', engine='xlsxwriter') as writer:\n",
    "        workbook = writer.book\n",
    "\n",
    "        for obj in all_objects:\n",
    "            title, subtitles, tables = extract_data(obj.get(\"content\", []))\n",
    "            update_history = obj.get(\"updateHistory\", [])  # Get updateHistory data if available\n",
    "            write_to_excel(title, subtitles, tables, update_history, workbook)\n",
    "\n",
    "        # Write status counts to the \"Statistics\" sheet\n",
    "        write_status_counts_to_excel(all_objects, workbook)\n",
    "\n",
    "    print(\"Excel file saved successfully.\")\n",
    "\n",
    "extract_and_write_to_excel(all_objects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "tagList = [\"GAP_GAP04.PLC04.MLD1_DATA_Anode_Geometric\"]\n",
    "\n",
    "def getValues4(tagList, startDate, endDate):\n",
    "    url =  \"https://data.exactspace.co/kairosapi/api/v1/datapoints/query\"\n",
    "    d = {\n",
    "        \"metrics\": [\n",
    "            {\n",
    "                \"tags\": {},\n",
    "                \"name\": \"\",\n",
    "            }\n",
    "        ],\n",
    "        \"plugins\": [],\n",
    "        \"cache_time\": 0,\n",
    "        \"start_absolute\": startDate,\n",
    "        \"end_absolute\": endDate\n",
    "    }\n",
    "    dfs = []\n",
    "    for tag in tagList:\n",
    "        d['metrics'][0]['name'] = tag\n",
    "        res = requests.post(url=url, json=d)\n",
    "        values = json.loads(res.content)\n",
    "        df = pd.DataFrame(values[\"queries\"][0][\"results\"][0]['values'], columns=['time', values[\"queries\"][0][\"results\"][0]['name']])\n",
    "        df['time'] = pd.to_datetime(df['time'], unit='ms') + pd.Timedelta(hours=5.5)\n",
    "        df.sort_values(by='time', inplace=True)\n",
    "        df = df.drop_duplicates(subset=[\"time\", tag])\n",
    "        if df.shape[0] < 10:\n",
    "            pass\n",
    "        else:\n",
    "            dfs.append(df)\n",
    "    df = dfs[0]\n",
    "    for df_ in dfs[1:]:\n",
    "        df = pd.merge(df, df_, on='time')\n",
    "    return df\n",
    "\n",
    "# Function to convert ISO 8601 timestamp to epoch timestamp\n",
    "def iso_to_epoch(iso_timestamp):\n",
    "    return int(datetime.fromisoformat(iso_timestamp[:-1]).timestamp())\n",
    "\n",
    "# Extract recommendation data from JSON and create DataFrame\n",
    "def extract_recommendations(all_objects):\n",
    "    recommendations = []\n",
    "    for obj in all_objects:\n",
    "        title = obj.get(\"content\", [{}])[0].get(\"value\", \"\")\n",
    "        if \"GAP: GSD Recommendations\" in title:\n",
    "            recommendation_text = obj.get(\"content\", [{}])[1].get(\"value\", \"\")  # Assuming the recommendation text is in the second item of the content list\n",
    "            update_history = obj.get(\"updateHistory\", [])\n",
    "            update_actions = [entry.get(\"action\", \"\") for entry in update_history]  # Extract actions from updateHistory\n",
    "            actions_str = \"\\n\".join(update_actions)  # Concatenate actions into a single string separated by newline\n",
    "            created_time = obj.get(\"createdOn\")\n",
    "            due_time = obj.get(\"dueDate\")\n",
    "            startDate = iso_to_epoch(created_time) * 1000\n",
    "            endDate = iso_to_epoch(due_time) * 1000\n",
    "            # Fetch data for the specified time range\n",
    "            data = getValues4(tagList, startDate, endDate)\n",
    "            data = data.tail(1)\n",
    "            data = data.round(4)\n",
    "            data = data.drop(columns=['time'])\n",
    "            density = data.to_dict('records')\n",
    "            result = density[0].get(\"GAP_GAP04.PLC04.MLD1_DATA_Anode_Geometric\")\n",
    "            if result > 1.65:\n",
    "                density_achieved = \"Yes\"\n",
    "            else:\n",
    "                density_achieved = \"No\"\n",
    "            recommendations.append({\"ID\": obj.get(\"id\", \"\"), \"Recommendation\": recommendation_text, \"Density Achieved\": density_achieved, \"Update Actions\": actions_str})\n",
    "    return recommendations\n",
    "\n",
    "# Extract data from JSON and write to Excel\n",
    "def extract_and_write_to_excel(all_objects):\n",
    "    recommendations = extract_recommendations(all_objects)\n",
    "    if not recommendations:\n",
    "        print(\"No recommendations found.\")\n",
    "        return\n",
    "    \n",
    "    df = pd.DataFrame(recommendations)\n",
    "    \n",
    "    # Define writer\n",
    "    writer = pd.ExcelWriter(\"recommendations.xlsx\", engine=\"xlsxwriter\")\n",
    "    df.to_excel(writer, index=False, sheet_name=\"Recommendations\")\n",
    "    worksheet = writer.sheets[\"Recommendations\"]\n",
    "    \n",
    "    # Set column width\n",
    "    worksheet.set_column('A:Z', 15)  # Set column width for ID\n",
    "    \n",
    "    # Increase row height for cells with long text\n",
    "    for i, text in enumerate(df[\"Recommendation\"], start=1):\n",
    "        num_lines = text.count(\"\\n\") + 1  # Count the number of lines in the text\n",
    "        if num_lines > 1:\n",
    "            worksheet.set_row(i, 20 * num_lines)  # Set row height to accommodate the text\n",
    "        \n",
    "    writer.save()\n",
    "    print(\"Excel file saved successfully.\")\n",
    "\n",
    "# Assuming `all_objects` is defined somewhere\n",
    "extract_and_write_to_excel(all_objects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import schedule\n",
    "import gc\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests , json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import pytz\n",
    "from pytz import timezone\n",
    "import platform\n",
    "import pytz\n",
    "from datetime import timedelta\n",
    "# from datetime import datetime\n",
    "import datetime as datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_shiftwise():\n",
    "    taglist = ['GAP_GAP04.PLC04.MLD1_DATA_Anode_Number','GAP_GAP04.PLC04.MLD1_DATA_Anode_Geometric','GAP_GAP04.PLC04.MLD1_DATA_Anode_Dry_Density','GAP_GAP04.PLC04.MLD1_DATA_Anode_Weight','GAP_GAP04.PLC04.MLD1_DATA_Anode_Height']\n",
    "    taglist1 = ['GAP_GAP04.PLC04.MLD2_DATA_Anode_Number','GAP_GAP04.PLC04.MLD2_DATA_Anode_Geometric','GAP_GAP04.PLC04.MLD2_DATA_Anode_Dry_Density','GAP_GAP04.PLC04.MLD2_DATA_Anode_Weight','GAP_GAP04.PLC04.MLD2_DATA_Anode_Height']\n",
    "    taglist2 = ['GAP_GAP03.PLC03.ACTUAL_FORMULA.FKTP','GAP_GAP04.PLC04.U363_K145_FIT_01_PV','GAP_GAP04.PLC04.level_2_production_reject']\n",
    "    checklist = ['GAP_GAP03.PLC03.SCHENCK2_FEED_RATE','GAP_GAP04.PLC04.MLD1_DATA_Anode_Number','GAP_GAP04.PLC04.MLD2_DATA_Anode_Number']\n",
    "    process_tags = [\n",
    "    \"GAP_GAP03.PLC03.ACTUAL_FORMULA.KGS\",\n",
    "    \"GAP_GAP03.PLC03.ACTUAL_FORMULA.KLP\",\n",
    "    \"GAP_GAP03.PLC03.ACTUAL_FORMULA.KFR\",\n",
    "    \"GAP_GAP01.PLC01.U362_E020_MVF_01_ACTRL_AUTOSPEEDREF\",\n",
    "    \"GAP_GAP01.PLC01._GAPPOS2.PV\",\n",
    "    \"GAP_GAP03.PLC03._362_J150_WIT_01.PV\", # Mixer weight\n",
    "    \"GAP_GAP03.PLC03.J362_J150_JT_01_PW01_IN\", # mixer load\n",
    "    \"GAP_GAP03.PLC03._362_J155_WIT_01.PV\",  # paste coller weight\n",
    "    \"GAP_GAP03.PLC03.J362_J155_JT_01_PW01_IN\", # cooler load\n",
    "    \"GAP_GAP04.PLC04.U363_K010_TT_01_PV\",\n",
    "    \"GAP_GAP04.PLC04.K363_K040A_MVF_01_VTK\",\n",
    "    \"GAP_GAP04.PLC04.MLD1_DATA_Anode_Vaccum_Pres\",\n",
    "    \"GAP_GAP04.PLC04.MLD2_DATA_Anode_Vaccum_Pres\",\n",
    "    \"GAP_GAP04.PLC04.MLD1_DATA_Anode_Counter_Pres\",\n",
    "    \"GAP_GAP04.PLC04.MLD2_DATA_Anode_Counter_Pres\",\n",
    "    \"GAP_GAP03.PLC03._362_J150B_JT_01.PV\", #Mixer pan motor 1 power\n",
    "    \"GAP_GAP03.PLC03._362_J150B_JT_02.PV\", #Mixer pan motor 2 power\n",
    "    \"GAP_GAP03.PLC03.U362_J155B_JT_01_PV\", #Cooler pan motor 1 power\n",
    "    \"GAP_GAP03.PLC03.U362_J155B_JT_02_PV\"] #Cooler pan motor 2 power\n",
    "\n",
    "    def fetch_data_with_retry(url, payload, max_retries=10, retry_delay=2):\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                res = requests.post(url=url, json=payload)\n",
    "                res.raise_for_status()  # Raise an exception for 4xx or 5xx status codes\n",
    "                return res\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Failed to fetch data (attempt {attempt + 1}/{max_retries}): {e}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(f\"Retrying in {retry_delay} seconds...\")\n",
    "                    time.sleep(retry_delay)\n",
    "        raise RuntimeError(\"Failed to fetch data after multiple attempts\")\n",
    "\n",
    "    def getValues(tagList):\n",
    "        # url =config[\"60ae9143e284d016d3559dfb\"][\"api\"][\"query\"]\n",
    "        url='https://data.exactspace.co/kairosapi/api/v1/datapoints/query'\n",
    "        # print(url)\n",
    "        d = {\n",
    "            \"metrics\": [\n",
    "                {\n",
    "                    \"tags\": {},\n",
    "                    \"name\": \"\",\n",
    "                    \"aggregators\": [\n",
    "                        {\n",
    "                            \"name\": \"avg\",\n",
    "                            \"sampling\": {\n",
    "                                \"value\": \"1\",\n",
    "                                \"unit\": \"minutes\"\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            \"plugins\": [],\n",
    "            \"cache_time\": 0,\n",
    "            \"start_relative\": {\n",
    "                \"value\": \"1\",\n",
    "                \"unit\": \"months\"\n",
    "            }\n",
    "        }\n",
    "        finalDF = pd.DataFrame()\n",
    "        for tag in tagList:\n",
    "            d['metrics'][0]['name'] = tag\n",
    "            try:\n",
    "                res = fetch_data_with_retry(url, d)\n",
    "                values = json.loads(res.content)\n",
    "                df = pd.DataFrame(values[\"queries\"][0][\"results\"][0]['values'], columns=['time', values[\"queries\"][0][\"results\"][0]['name']])\n",
    "                finalDF = pd.concat([finalDF, df], axis=1)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to fetch data for tag: {tag}. Error: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # finalDF.rename(columns=tagList, inplace=True) \n",
    "        finalDF = finalDF.loc[:, ~finalDF.columns.duplicated()]\n",
    "        finalDF.dropna(subset=['time'], inplace=True)\n",
    "        finalDF['time'] = pd.to_datetime(finalDF['time'], unit='ms') + pd.Timedelta(hours=5.5)\n",
    "        return finalDF\n",
    "        \n",
    "    # print(check_data.head())\n",
    "    # def status_check():\n",
    "    #     data= getValues(checklist)\n",
    "    #     print(data.shape)\n",
    "    #     data = data[(data['GAP_GAP04.PLC04.MLD1_DATA_Anode_Number'] % 1 == 0)]\n",
    "    #     data = data[data['GAP_GAP04.PLC04.MLD1_DATA_Anode_Number'] != data['GAP_GAP04.PLC04.MLD1_DATA_Anode_Number'].shift()]\n",
    "    #     if len(data)>5:\n",
    "    #         return True\n",
    "    #     else:\n",
    "    #         return False\n",
    "    plant_status = True\n",
    "    # print(plant_status)\n",
    "\n",
    "    ##plant_status is equal to true\n",
    "    if plant_status:\n",
    "        data = getValues(taglist)\n",
    "        data1 = getValues(taglist1)\n",
    "        data2 = getValues(taglist2)\n",
    "        process_parameters = getValues(process_tags)\n",
    "        def Current_shift_KPIs(data, data1, data2,process_parameters):\n",
    "            def convert_values_to_string(dataframes_list):\n",
    "                return [df.applymap(str) for df in dataframes_list]\n",
    "            Process_tags_name = [\"Time\",\"Green Scrap in formula (%)\",\"Pitch in formula (%)\",\"Fines in formula(%)\",\"Rhodax speed (m/s)\",\"Rhodax Gap (mm)\",\"Mixer weight (kg)\",\"Mixer Rotor Power (kw)\",\"Cooler weight (kg)\",\"Cooler Rotor Power (Kw)\",\"Paste Temperature (deg)\",\"Average Vibration Time (seconds)\",\"Average Vaccum Pressure Mould 1 (mbar) \",\"Average Vaccum Pressure Mould 2 (mbar)\",\"Counter Pressure Mould 1 (mbar)\",\"Counter Pressure Mould 2 (mbar)\",\"Mixer P1\",\"Mixer P2\",\"Cooler P1\",\"Cooler P2\"]\n",
    "\n",
    "            # Filter and process data\n",
    "            data = data[(data['GAP_GAP04.PLC04.MLD1_DATA_Anode_Number'] % 1 == 0)]\n",
    "            data = data[data['GAP_GAP04.PLC04.MLD1_DATA_Anode_Number'] != data['GAP_GAP04.PLC04.MLD1_DATA_Anode_Number'].shift()]\n",
    "\n",
    "            data1 = data1[(data1['GAP_GAP04.PLC04.MLD2_DATA_Anode_Number'] % 1 == 0)]\n",
    "            data1 = data1[data1['GAP_GAP04.PLC04.MLD2_DATA_Anode_Number'] != data1['GAP_GAP04.PLC04.MLD2_DATA_Anode_Number'].shift()]\n",
    "            data.columns = ['Time', 'Mould1_anode_number', 'Mould1_Geometric_density', 'Mould1_Dry_density','Mould1_Anode_weight','Mould1_Anode_Height']\n",
    "            data1.columns = ['Time', 'Mould2_anode_number', 'Mould2_Geometric_density', 'Mould2_Dry_density','Mould2_Anode_weight','Mould2_Anode_Height']\n",
    "            data2.columns = ['Time', 'Total Paste', 'Paste Rejection','Total paste production reject']\n",
    "            \n",
    "            column_name_M1 = 'Mould1_Anode_weight'\n",
    "            count_below_1050_M1 = (data[column_name_M1] < 1050).sum()\n",
    "\n",
    "            column_name_M2 = 'Mould2_Anode_weight'\n",
    "            count_below_1050_M2 = (data1[column_name_M2] < 1050).sum()\n",
    "\n",
    "            column_name_Weight_M1 = 'Mould1_Anode_Height'\n",
    "            count_below_680_M1 = (data[column_name_Weight_M1] < 680).sum()\n",
    "\n",
    "            column_name_Weight_M2 = 'Mould2_Anode_Height'\n",
    "            count_below_680_M2 = (data1[column_name_Weight_M2] < 680).sum()\n",
    "            \n",
    "            Mould1_anode_Height = data['Mould1_Anode_Height'].mean().round(3)\n",
    "            Mould2_anode_Height = data1['Mould2_Anode_Height'].mean().round(3)\n",
    "            Anode_height = (Mould1_anode_Height + Mould2_anode_Height)/2\n",
    "            Anode_height = Anode_height.round(3)\n",
    "\n",
    "            percentage_w1 = (count_below_1050_M1 / len(data['Mould1_Anode_weight'])) * 100\n",
    "            percentage_w2 = (count_below_1050_M2 / len(data1['Mould2_Anode_weight'])) * 100\n",
    "            percentage_h1 = (count_below_680_M1 / len(data['Mould1_Anode_Height'])) * 100\n",
    "            percentage_h2 = (count_below_680_M2 / len(data1['Mould2_Anode_Height'])) * 100\n",
    "            \n",
    "            Anode1_M1 = (data['Mould1_Geometric_density'] > 1.66 ).sum()\n",
    "            Anode1_M2 = (data1['Mould2_Geometric_density'] > 1.66 ).sum()\n",
    "            Anode2_M1 = ((data['Mould1_Geometric_density'] >= 1.65) & (data['Mould1_Geometric_density'] <= 1.66)).sum()\n",
    "            Anode2_M2 = ((data1['Mould2_Geometric_density'] >= 1.65) & (data1['Mould2_Geometric_density'] <= 1.66)).sum()\n",
    "            Anode3_M1 = (data['Mould1_Geometric_density'] < 1.65 ).sum()\n",
    "            Anode3_M2 = (data1['Mould2_Geometric_density'] < 1.65 ).sum()\n",
    "\n",
    "\n",
    "            # percentage_data = pd.DataFrame({\n",
    "            #     \"Parameters\": [\"Weight less than 1050(Mould1)\", \"Weight less than 1050(Mould2)\", \"Height less than 680(Mould1)\", \"Height less than 680(Mould2)\"],\n",
    "            #     \"No of Anodes\": [f' {count_below_1050_M1} anodes', f' {count_below_1050_M2} anodes', f' {count_below_680_M1} anodes', f' {count_below_680_M2}  anodes']\n",
    "            # })\n",
    "\n",
    "            quality_metrics_data = pd.DataFrame({\n",
    "                #\"Anode Quality Metrics\":[\">1.66 in Mould 1\",\">1.66 in Mould 2\",\"1.65 to 1.66 in Mould 1\",\"1.65 to 1.66 in Mould 2\",\"<1.65 in Mould1 \",\"<1.65 in Mould2\"],\n",
    "                \"Anode Quality Metrics\" :['Mould 1 (no of anodes)','Mould 2 (no of anodes)'],\n",
    "                \"Density: >1.66\":[f'{Anode1_M1}',f'{Anode1_M2}'],\n",
    "                \"Density: 1.65 to 1.66\":[f'{Anode2_M1} ',f'{Anode2_M2}'],\n",
    "                \"Density: < 1.65\":[f'{Anode3_M1} ',f'{Anode3_M2} '],\n",
    "                \"Weight : < 1050\":[f' {count_below_1050_M1} ', f' {count_below_1050_M2} '],\n",
    "                \"Height : < 680 mm\":[f' {count_below_680_M1} ', f' {count_below_680_M2}  '],\n",
    "            })\n",
    "            \n",
    "            \n",
    "            process_parameters.columns = Process_tags_name\n",
    "            \n",
    "            # ist = timezone('Asia/Kolkata')\n",
    "            # now = datetime.datetime.now(ist)\n",
    "\n",
    "            # current_hour = now.hour\n",
    "            # #print(current_hour)\n",
    "            # # Define the shift based on the current hour\n",
    "            # if 23 <= current_hour or current_hour < 7:\n",
    "            #     shift_name = 'Shift B data'\n",
    "            # elif 7 <= current_hour < 15:\n",
    "            #     shift_name = 'Shift C data'\n",
    "            # elif 15 <= current_hour < 23:\n",
    "            #     shift_name = 'Shift A data'\n",
    "                \n",
    "            shift_name = \"Shift Report\"\n",
    "            process_parameters = process_parameters[(process_parameters['Rhodax Gap (mm)'] > 16) & (process_parameters['Rhodax Gap (mm)'] < 25)]\n",
    "            process_parameters = process_parameters[(process_parameters['Paste Temperature (deg)']>160) & (process_parameters['Paste Temperature (deg)']<176)]\n",
    "            # process_parameters['Mixer weight (kg)']=process_parameters['Mixer weight (kg)']/907.2\n",
    "            # process_parameters['Cooler weight (kg)']=process_parameters['Cooler weight (kg)']/907.2\n",
    "            process_parameters['Mixer specific power (Kw/Kg)']=(process_parameters['Mixer P1']+process_parameters['Mixer P2'])/process_parameters['Mixer weight (kg)']\n",
    "            process_parameters['Cooler specific power (Kw/Kg)']=(process_parameters['Cooler P1']+process_parameters['Cooler P2'])/process_parameters['Cooler weight (kg)']\n",
    "            # process_parameters['Specific Power (Kw/Ton)'] = ((process_parameters['Mixer P1']+process_parameters['Mixer P2'])+(process_parameters['Cooler P1']+process_parameters['Cooler P2'])) / data2['Total Paste']\n",
    "            filtered_columns = [col for col in process_parameters.columns if 'P1' not in col and 'P2' not in col and 'Time' not in col]\n",
    "            average_values = process_parameters[filtered_columns].mean()\n",
    "            Process_parameters_table = pd.DataFrame({'Process Parameters': average_values.index, shift_name : average_values.values})\n",
    "            Process_parameters_table = Process_parameters_table.round(3)\n",
    "            # print(Process_parameters_table)\n",
    "\n",
    "\n",
    "            # data2.loc[data2['Paste Rejection'] < 0.2, 'Paste Rejection'] = 0\n",
    "            data = data[data['Mould1_Anode_weight']>1000]\n",
    "            data1 = data1[data1['Mould2_Anode_weight'] > 1000]\n",
    "            \n",
    "            data.reset_index(drop=True, inplace=True)\n",
    "            data2.reset_index(drop=True, inplace=True)\n",
    "            data1.reset_index(drop=True, inplace=True)\n",
    "   \n",
    "            data2['Total Paste'] = data2['Total Paste'] / 60\n",
    "            first_value=data2['Total paste production reject'].iloc[0]\n",
    "            last_value=data2['Total paste production reject'].iloc[-1]\n",
    "            Total_rejected_paste=(abs(last_value-first_value))/907.2\n",
    "            # print(Total_rejected_paste)\n",
    "            Total_rejected_paste=Total_rejected_paste.round(2)\n",
    "            Total_paste_sum=data2['Total Paste'].sum()\n",
    "            paste_rejection_percentage=(Total_rejected_paste/Total_paste_sum)*100\n",
    "            paste_rejection_percentage=paste_rejection_percentage.round(2)\n",
    "            Total_rejected_paste=str(Total_rejected_paste)+'tons'\n",
    "\n",
    "            Mould1_anode_weight_mean = data['Mould1_Anode_weight'].mean().round(0)\n",
    "            Mould2_anode_weight_mean = data1['Mould2_Anode_weight'].mean().round(0)\n",
    "            Mould1_anode_weight_stdv = data['Mould1_Anode_weight'].std().round(3)\n",
    "            Mould2_anode_weight_stdv = data1['Mould2_Anode_weight'].std().round(3)\n",
    "            Mould1_Geometric_mean = data['Mould1_Geometric_density'].mean().round(3)\n",
    "            Mould2_Geometric_mean = data1['Mould2_Geometric_density'].mean().round(3)\n",
    "            Mould1_Geometric_stdv = data['Mould1_Geometric_density'].std().round(3)\n",
    "            Mould2_Geometric_stdv = data1['Mould2_Geometric_density'].std().round(3)\n",
    "            Mould1_Dry_mean = data['Mould1_Dry_density'].mean().round(3)\n",
    "            Mould2_Dry_mean = data1['Mould2_Dry_density'].mean().round(3)\n",
    "            Mould1_Dry_stdv = data['Mould1_Dry_density'].std().round(3)\n",
    "            Mould2_Dry_stdv = data1['Mould2_Dry_density'].std().round(3)\n",
    "            Anode_weight = (int(Mould1_anode_weight_mean) +int(Mould2_anode_weight_mean))/2\n",
    "            Anode_weight = int(Anode_weight)\n",
    "            combined_mean_Geometric = pd.concat([data['Mould1_Geometric_density'], data1['Mould2_Geometric_density']]).mean().round(3)\n",
    "            combined_std_dev_Geometric = pd.concat([data['Mould1_Geometric_density'], data1['Mould2_Geometric_density']]).std().round(3)\n",
    "            combined_std_dev_Dry = pd.concat([data['Mould1_Dry_density'], data1['Mould2_Dry_density']]).std().round(3)\n",
    "            # print(combined_std_dev_Dry)\n",
    "            combined_mean_dry = pd.concat([data['Mould1_Dry_density'], data1['Mould2_Dry_density']]).mean().round(3)\n",
    "            combined_std_dev_weight = pd.concat([data['Mould1_Anode_weight'], data1['Mould2_Anode_weight']]).std().round(3)\n",
    "            \n",
    "            table_data1 = {\n",
    "                'GA Density (g/cm^3)': [combined_mean_Geometric],\n",
    "                'GA Dry Density(g/cm^3)': [combined_mean_dry],\n",
    "                'GA Density Stdv': [combined_std_dev_Geometric],\n",
    "                'GA weight Stdv': [combined_std_dev_weight],\n",
    "                'Green Paste Rejection (%)': [paste_rejection_percentage]\n",
    "            }\n",
    "            table_data = {\n",
    "                'GA Density (g/cm^3)': [combined_mean_Geometric],\n",
    "                'GA Dry Density(g/cm^3)': [combined_mean_dry],\n",
    "                'GA Density Stdv': [combined_std_dev_Geometric],\n",
    "                'GA Weight':[Anode_weight],\n",
    "                'GA weight Stdv': [combined_std_dev_weight],\n",
    "                'Green Paste Rejection (%)': [f' {paste_rejection_percentage}({Total_rejected_paste})'],\n",
    "                'GA Height (mm)': [Anode_height]\n",
    "            }\n",
    "\n",
    "            # Create a DataFrame from the dictionary\n",
    "            result_table = pd.DataFrame(table_data)\n",
    "            result_table1 =pd.DataFrame(table_data1)\n",
    "            # print(result_table)\n",
    "            \n",
    "            # Modify the column names accordingly\n",
    "            result_table = result_table.T\n",
    "            result_table1 = result_table1.T \n",
    "            \n",
    "            result_table['Targets'] = [ '>=1.650', '>=1.4270', '<=0.005',1050, '< 4','<2.2', 680]\n",
    "            result_table1['Targets'] = [1.650, 1.4270, 0.005, 4,2.2]\n",
    "            result_table['Mould 1 Data'] = [Mould1_Geometric_mean,Mould1_Dry_mean,Mould1_Geometric_stdv,Mould1_anode_weight_mean,Mould1_anode_weight_stdv,'-',Mould1_anode_Height]\n",
    "            result_table['Mould 2 Date'] = [Mould2_Geometric_mean,Mould2_Dry_mean,Mould2_Geometric_stdv,Mould2_anode_weight_mean,Mould2_anode_weight_stdv,'-',Mould2_anode_Height]\n",
    "\n",
    "            result_table = result_table.rename(columns={0: shift_name})\n",
    "            result_table1 = result_table1.rename(columns={0: shift_name})\n",
    "\n",
    "            result_table.reset_index(inplace=True)\n",
    "            result_table1.reset_index(inplace =True)\n",
    "            result_table.columns = ['KPIs',shift_name,\"Targets\",'Mould 1 Data','Mould 2 Data']\n",
    "            result_table1.columns = ['KPIs',shift_name,\"Targets\"]\n",
    "            \n",
    "                                \n",
    "            result_table = result_table[['KPIs', 'Targets','Mould 1 Data','Mould 2 Data',shift_name]]\n",
    "            result_table1 = result_table1[['KPIs', 'Targets', shift_name]]\n",
    "            result_table = result_table.round(3)\n",
    "            result_table1 = result_table1.round(3)\n",
    "            df = result_table1.copy()\n",
    "            # print(df.head())\n",
    "            result_table = result_table\n",
    "            #print(result_table)\n",
    "            # Determine colors based on whether the target is met\n",
    "            conditions = ['>=', '>=', '<=', '<=', '<=']\n",
    "            df['Color'] = ['green' if ((cond == '>=' and current >= target) or\n",
    "                                            (cond == '<=' and current <= target))\n",
    "                        else 'salmon' for current, target, cond in zip(df[shift_name], df['Targets'], conditions)]\n",
    "            display_frames = [result_table]\n",
    "            display_frames.append(quality_metrics_data)\n",
    "            display_frames.append(Process_parameters_table)\n",
    "            # Convert all values in each DataFrame to strings\n",
    "            display_frames = convert_values_to_string(display_frames)\n",
    "            return display_frames,df\n",
    "\n",
    "        def plot_kpi_targets(df):\n",
    "            DIRECTORY_PATH='kpi_target_img'\n",
    "            #print(DIRECTORY_PATH)\n",
    "            ist = timezone('Asia/Kolkata')\n",
    "            now = datetime.datetime.now(ist)\n",
    "            current_hour = now.hour\n",
    "\n",
    "            if 23 <= current_hour or current_hour < 7:\n",
    "                shift_name = 'Shift B data'\n",
    "            elif 7 <= current_hour < 15:\n",
    "                shift_name = 'Shift C data'\n",
    "            elif 15 <= current_hour < 23:\n",
    "                shift_name = 'Shift A data'\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(15, 9))\n",
    "            bar_width = 0.35\n",
    "            index = pd.Index(range(len(df['KPIs'])))\n",
    "\n",
    "            current_bars = ax.bar(index - bar_width/2, df[shift_name], bar_width, label= shift_name , color=df['Color'])\n",
    "            target_bars = ax.bar(index + bar_width/2, df['Targets'], bar_width, label='Targets', color='lightgreen')\n",
    "\n",
    "            for bar in current_bars + target_bars:\n",
    "                height = bar.get_height()\n",
    "                ax.annotate('{}'.format(height),\n",
    "                            xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                            xytext=(0, 3),\n",
    "                            textcoords=\"offset points\",\n",
    "                            ha='center', va='bottom')\n",
    "            ax.legend(handles=[plt.Line2D([0], [0], color='salmon', label='Target Not Achieved'),plt.Line2D([0], [0], color='lightgreen', label='Targets'),plt.Line2D([0], [0], color='green', label=shift_name)])\n",
    "            ax.set_xlabel('KPIs')\n",
    "            ax.set_ylabel('Values')\n",
    "            ax.set_title( shift_name + ' vs Targets')\n",
    "            ax.set_xticks(index)\n",
    "            ax.set_xticklabels(df['KPIs'], rotation=45, ha='right')\n",
    "            #ax.legend()\n",
    "            print(\"Saving images to:\", DIRECTORY_PATH)\n",
    "            if not os.path.exists(DIRECTORY_PATH):\n",
    "               os.makedirs(DIRECTORY_PATH)\n",
    "            image_path = os.path.join(DIRECTORY_PATH, 'plot_image.png')\n",
    "            try:\n",
    "                plt.savefig(image_path)\n",
    "                plt.close()  # Close the plot to free up memory\n",
    "                print(\"Image saved at:\", image_path)\n",
    "            except Exception as e:\n",
    "                print(\"Error saving image:\", e) \n",
    "            return image_path\n",
    "        \n",
    "        dataframes,df = Current_shift_KPIs(data, data1, data2,process_parameters)\n",
    "        df = df.round(3)\n",
    "        image_path = plot_kpi_targets(df)\n",
    "        print(image_path)\n",
    "        \n",
    "        def uploadRefernceData(fileName):\n",
    "            #print(fileName)\n",
    "            #print(type(fileName))\n",
    "            str_fileName = str(fileName)\n",
    "            \n",
    "            path = \"\"\n",
    "            files = {'upload_file': open(str(path+str_fileName),'rb')}\n",
    "            #print(files)\n",
    "            url='https://data.exactspace.co/exactapi/attachments/tasks/upload'\n",
    "            response = requests.post(url, files=files)\n",
    "            print (\"uploading\")\n",
    "            #print (url)\n",
    "            #print (\"+\"*20)\n",
    "\n",
    "            if(response.status_code==200):\n",
    "                status =\"success\"\n",
    "                data = response.content\n",
    "                # Parse the JSON data\n",
    "                parsed_data = json.loads(data)\n",
    "                # Access the \"name\" from the parsed JSON data\n",
    "                name = parsed_data['result']['files']['upload_file'][0]['name']\n",
    "                return \"https://data.exactspace.co/exactapi/attachments/tasks/download/\"+name\n",
    "\n",
    "                \n",
    "            else:\n",
    "                status= (str(response.status_code) + str(response.content))\n",
    "                print (response.status_code, response.content)\n",
    "\n",
    "            return status\n",
    "        image_link = uploadRefernceData(image_path)\n",
    "        print(\"this is uploaded image link:\", image_link)\n",
    "        current_date_time = datetime.datetime.now() + pd.Timedelta(hours=5.5)\n",
    "        task_creation_time = current_date_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        json_body = {\n",
    "            \"type\": \"task\",\n",
    "                \"voteAcceptCount\": 0,\n",
    "                \"voteRejectCount\": 0,\n",
    "                \"acceptedUserList\": [],\n",
    "                \"rejectedUserList\": [],\n",
    "                \"dueDate\": \"2023-11-21T16:00:00.391Z\",\n",
    "                \"assignee\": \"6149b9acf1902b2b7aecf9b1\",#anisha\n",
    "                # \"assignee\":'632d3bd36d161904360db797', #intern\n",
    "                \"source\": \"Anode Forming\",\n",
    "                \"team\": \"GAP Shift Report\",\n",
    "                \"createdBy\": \"5f491bb942ba5c3f7a474d15\",\n",
    "                \"createdOn\":  \"2023-11-21T14:49:03.633Z\",\n",
    "                \"lastUpdatedOn\":\"2023-11-21T14:49:03.633Z\",\n",
    "                \"siteId\": \"60ae7260e284d016d3559d09\",\n",
    "                \"unique\":\"KPIs\",\n",
    "                \"subTasks\": [],\n",
    "                \"chats\": [],\n",
    "                \"taskPriority\": \"high\",\n",
    "                \"updateHistory\":[{\n",
    "                    \"action\":\"This task is created by Pulse.\",\n",
    "                     \"by\": \"\",\n",
    "                     \"on\": task_creation_time\n",
    "                }],\n",
    "                \"unitsId\": \"60ae9143e284d016d3559dfb\",\n",
    "                \"collaborators\": [\n",
    "                    \"632d3bd36d161904360db797\" #intern\n",
    "                    '5c591d697dc9e324ee08a456', \n",
    "                    '61431baf1c46e3435ff50ac7', #sayan sir \n",
    "                    '5f491bb942ba5c3f7a474d15', \n",
    "                    '6149b95af1902b2b7aecf9ac', # anurag sir\n",
    "                    '6542666c3a469b00079087e8', # aswini mam\n",
    "                    '6542668db3e4990006874e9b', # dibendu sir\n",
    "                    '6561adf4bd95b70007098856', #satyanand sir\n",
    "                    '6561ae15c417de00073b62f7', #saurab sir\n",
    "                    '6561ae33c417de00073b62f8', #akhilesh sir\n",
    "                    '6565801507703c0007d79240', #arunk sir\n",
    "                    '65a6356760b4b400074bbf41', #jitendra sir\n",
    "                    '65a6379c092148000766155b', #atul sir\n",
    "                    '65f180008afd390007b693ee', #sushil sir\n",
    "                    '65f180390ef6370007eb399a' #rahul sir\n",
    "                ],\n",
    "                \"status\": \"inprogress\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"title\",\n",
    "                        \"value\": \"\"\n",
    "                    }\n",
    "                ],\n",
    "                \"taskGeneratedBy\": \"system\",\n",
    "                \"incidentId\": \"\",\n",
    "                \"category\": \"\",\n",
    "                \"sourceURL\": \"\",\n",
    "                \"notifyEmailIds\": [\n",
    "                ],\n",
    "                \"chat\": [],\n",
    "                \"taskDescription\": \"<p><img src=\\\"{}\\\"></p>\".format(image_link),\n",
    "                \"triggerTimePeriod\": \"days\",\n",
    "                \"viewedUsers\": [],\n",
    "                \"completedBy\": \"\",\n",
    "                \"equipmentIds\": [],\n",
    "                \"mentions\": [],\n",
    "                \"systems\": []\n",
    "            }\n",
    "        \n",
    "        def update_json(json_body, dataframes):\n",
    "            # Get the current time in IST\n",
    "            ist = timezone('Asia/Kolkata')\n",
    "            now = datetime.datetime.now(ist)\n",
    "            due_date = now + timedelta(hours=5)\n",
    "            json_body[\"dueDate\"] = due_date.replace(tzinfo=None).isoformat()  # Remove the timezone info\n",
    "            json_body[\"createdOn\"] = now.replace(tzinfo=None).isoformat()  # Remove the timezone info\n",
    "            current_hour = now.hour\n",
    "            #print(current_hour)\n",
    "            current_date = datetime.datetime.now().strftime(\"%d-%m-%Y\")\n",
    "\n",
    "            # Calculate the previous day's date\n",
    "            previous_day_date = (datetime.datetime.now() - datetime.timedelta(days=1)).strftime(\"%d-%m-%Y\")\n",
    "\n",
    "            # Your existing logic for determining the title\n",
    "            global title\n",
    "            if 23 <= current_hour or current_hour < 7:\n",
    "                title = f'GAP Shift B KPI report ({current_date})'\n",
    "            elif 7 <= current_hour < 15:\n",
    "                title = f'GAP Shift C KPI report ({previous_day_date})'\n",
    "            elif 15 <= current_hour < 23:\n",
    "                title = f'GAP Shift A KPI report ({current_date})'\n",
    "        \n",
    "            json_body[\"content\"][0][\"value\"] = title\n",
    "            for dataframe in dataframes:\n",
    "                new_table_data = [dataframe.columns.tolist()] + dataframe.values.tolist()\n",
    "                json_body[\"content\"].append({\"type\": \"table\", \"value\": new_table_data})\n",
    "            return json_body\n",
    "        \n",
    "        def create_task_link(task_id):\n",
    "            try:\n",
    "                return 'https://data.exactspace.co/pulse-master/my-tasks/'+ task_id\n",
    "            except Exception as e:\n",
    "                print(\"error in creating the link\")\n",
    "            return\n",
    "        \n",
    "        def sendkpiEmail(taskdetails):\n",
    "            \n",
    "            # print(taskdetails)\n",
    "    \n",
    "            n = 1\n",
    "            unitName, SiteName, CustomerName = 'GAP', 'GAP Mahan', 'Green Anode Plant, Mahan'\n",
    "            \n",
    "            emailTemplate = os.path.join(os.getcwd(), 'kpiEmailTemplate.html')\n",
    "            \n",
    "            with open(emailTemplate, 'r') as f:\n",
    "                s = f.read()\n",
    "\n",
    "            # if PUBLIC_DATACENTER_URL != 'NA':\n",
    "            #     logoLink = 'img src=\"{}pulse-files/email-logos/logo.png\"'.format(PUBLIC_DATACENTER_URL)\n",
    "            #     s = s.replace('img src=\"#\"', logoLink)\n",
    "            # else:\n",
    "            logoLink ='https://data.exactspace.co/attachments/mail/download/logo.png'\n",
    "            s = s.replace('img src=\"#\"', 'img src=\"{}\"'.format(logoLink))\n",
    "\n",
    "                \n",
    "            s = s.replace(\"\"\"<a style=\"color: #fff; text-decoration:none;\" href=\"#\" id = 'task_link'>More Details</a>\"\"\", \n",
    "                          \"\"\"<a style=\"color: #fff; text-decoration:none;\" href=\"{}\" id = 'task_link'>More Details</a>\"\"\".format(taskdetails.get(\"task_link\")))\n",
    "\n",
    "            # print(taskdetails.get(\"link\"))\n",
    "            \n",
    "            s = s.replace('UnitName', unitName)\n",
    "            s = s.replace('SiteName', SiteName)\n",
    "            s = s.replace('CustomerName', CustomerName)\n",
    "            \n",
    "            devTable = ''\n",
    "            devTable = '<tbody id=\"devList\">'\n",
    "            try:\n",
    "                devTable += '''\n",
    "                    <tr>\n",
    "                        <td align=\"center\" width=\"40\" style=\"border-bottom: solid 1px #CACACA;\">{}</td>\n",
    "                        <td align=\"left\" style=\"font-size: 13px; border-bottom: solid 1px #CACACA;\">{}</td>\n",
    "                    </tr>\n",
    "                '''.format(n, taskdetails.get(\"desc\", ''))\n",
    "            except Exception as e:\n",
    "                print('Error in creating a table of alert', e)\n",
    "                return\n",
    "\n",
    "            s = s.replace('<tbody id=\"devList\">', devTable)\n",
    "\n",
    "            with open(os.path.join(os.getcwd(), 'almEmailTemp.html'), 'wb') as f:\n",
    "                f.write(s.encode('utf-8'))\n",
    "\n",
    "            with open(os.path.join(os.getcwd(), 'almEmailTemp.html'), 'r') as f:\n",
    "                msg_body = f.read()\n",
    "\n",
    "            try:\n",
    "                url = config['api']['meta'].replace('exactapi', 'mail/send-mail')\n",
    "                # print(url)\n",
    "                payload = json.dumps({\n",
    "                    'from': 'shashank.r@exactspace.co',\n",
    "                    'to': ['sayan.dey@adityabirla.com',\n",
    "                            'anurag.gaurav@adityabirla.com',\n",
    "                            'anisha.jonnalagadda@adityabirla.com',\n",
    "                            'aswini.mishra@adityabirla.com',\n",
    "                            'dibyendu.g@adityabirla.com',\n",
    "                            'satanand.vaishya@adityabirla.com',\n",
    "                            'sourabh.chourasiya@adityabirla.com',\n",
    "                            'akhilesh.prasad@adityabirla.com',\n",
    "                            'arunk.panday@adityabirla.com',\n",
    "                            'jitendra.prajapati@adityabirla.com',\n",
    "                            'atul.a.yadav@adityabirla.com',\n",
    "                            'sushil.yadav@adityabirla.com',\n",
    "                            'rahula.kumar@adityabirla.com'\n",
    "                    ],\n",
    "                    # 'to':['shashank.r@exactspace.co'],\n",
    "                    'html': msg_body,\n",
    "                    'cc': ['shashank.r@exactspace.co','sairam.g@exactspace.co','ashlin.f@exactspace.co','nikhil.s@exactpsace.co'],\n",
    "                    'subject': 'KPI Parameters Report',\n",
    "                    'body': msg_body\n",
    "                 })\n",
    "                headers = {'Content-Type': 'application/json'}\n",
    "                response = requests.post(url, data=payload, headers=headers)\n",
    "                # print(response)\n",
    "\n",
    "                if response.text == 'Success':\n",
    "                    return 'Success'\n",
    "                else:\n",
    "                    print('Error in sending mail', response.status_code)\n",
    "                    return 'Fail'\n",
    "\n",
    "            except Exception as e:\n",
    "                print('Error in sending mail', e)\n",
    "                return 'Fail'\n",
    "            finally:\n",
    "                s=None\n",
    "                msg_body=None\n",
    "                \n",
    "        updates_in_json = update_json(json_body,dataframes)\n",
    "        #print(updates_in_json)\n",
    "        json_data = json.dumps(updates_in_json)\n",
    "        print(json_data)\n",
    "\n",
    "        # Set the headers to indicate that you are sending JSON data\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        #print(data)\n",
    "        post_url='https://data.exactspace.co/exactapi/activities'\n",
    "        # Make the POST request\n",
    "        response = requests.post(post_url, data=json_data, headers=headers)\n",
    "        # Check the response\n",
    "        if response.status_code == 200:\n",
    "            print(\"Task Create request was successful\")\n",
    "            response_data = response.json()\n",
    "            # print(response_data)\n",
    "            global last_task_id\n",
    "            last_task_id = response_data.get('id')\n",
    "            # print(last_task_id)\n",
    "            task_link = create_task_link(last_task_id)\n",
    "            # print(task_link)\n",
    "            \n",
    "            sendkpiEmail({\"desc\":title,\"task_link\":task_link})\n",
    "\n",
    "        else:\n",
    "            print(\"Task Create request failed with status code:\", response.status_code)\n",
    "    else:\n",
    "        print(\"Plant is Shutdown\")\n",
    "    gc.collect()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "taglist = ['GAP_GAP04.PLC04.MLD1_DATA_Anode_Number','GAP_GAP04.PLC04.MLD1_DATA_Anode_Geometric','GAP_GAP04.PLC04.MLD1_DATA_Anode_Dry_Density','GAP_GAP04.PLC04.MLD1_DATA_Anode_Weight','GAP_GAP04.PLC04.MLD1_DATA_Anode_Height']\n",
    "taglist1 = ['GAP_GAP04.PLC04.MLD2_DATA_Anode_Number','GAP_GAP04.PLC04.MLD2_DATA_Anode_Geometric','GAP_GAP04.PLC04.MLD2_DATA_Anode_Dry_Density','GAP_GAP04.PLC04.MLD2_DATA_Anode_Weight','GAP_GAP04.PLC04.MLD2_DATA_Anode_Height']\n",
    "taglist2 = ['GAP_GAP03.PLC03.ACTUAL_FORMULA.FKTP','GAP_GAP04.PLC04.U363_K145_FIT_01_PV','GAP_GAP04.PLC04.level_2_production_reject']\n",
    "checklist = ['GAP_GAP03.PLC03.SCHENCK2_FEED_RATE','GAP_GAP04.PLC04.MLD1_DATA_Anode_Number','GAP_GAP04.PLC04.MLD2_DATA_Anode_Number']\n",
    "process_tags = [\n",
    "\"GAP_GAP03.PLC03.ACTUAL_FORMULA.KGS\",\n",
    "\"GAP_GAP03.PLC03.ACTUAL_FORMULA.KLP\",\n",
    "\"GAP_GAP03.PLC03.ACTUAL_FORMULA.KFR\",\n",
    "\"GAP_GAP01.PLC01.U362_E020_MVF_01_ACTRL_AUTOSPEEDREF\",\n",
    "\"GAP_GAP01.PLC01._GAPPOS2.PV\",\n",
    "\"GAP_GAP03.PLC03._362_J150_WIT_01.PV\", # Mixer weight\n",
    "\"GAP_GAP03.PLC03.J362_J150_JT_01_PW01_IN\", # mixer load\n",
    "\"GAP_GAP03.PLC03._362_J155_WIT_01.PV\",  # paste coller weight\n",
    "\"GAP_GAP03.PLC03.J362_J155_JT_01_PW01_IN\", # cooler load\n",
    "\"GAP_GAP04.PLC04.U363_K010_TT_01_PV\",\n",
    "\"GAP_GAP04.PLC04.K363_K040A_MVF_01_VTK\",\n",
    "\"GAP_GAP04.PLC04.MLD1_DATA_Anode_Vaccum_Pres\",\n",
    "\"GAP_GAP04.PLC04.MLD2_DATA_Anode_Vaccum_Pres\",\n",
    "\"GAP_GAP04.PLC04.MLD1_DATA_Anode_Counter_Pres\",\n",
    "\"GAP_GAP04.PLC04.MLD2_DATA_Anode_Counter_Pres\",\n",
    "\"GAP_GAP03.PLC03._362_J150B_JT_01.PV\", #Mixer pan motor 1 power\n",
    "\"GAP_GAP03.PLC03._362_J150B_JT_02.PV\", #Mixer pan motor 2 power\n",
    "\"GAP_GAP03.PLC03.U362_J155B_JT_01_PV\", #Cooler pan motor 1 power\n",
    "\"GAP_GAP03.PLC03.U362_J155B_JT_02_PV\"] #Cooler pan motor 2 power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaned and saved successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vikra\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:11: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "excel_file = 'recommendations.xlsx'\n",
    "df = pd.read_excel(excel_file)\n",
    "\n",
    "# Define the phrase to remove\n",
    "phrase_to_remove = \"This task is created by Pulse.\"\n",
    "\n",
    "# Remove the phrase from the specified column\n",
    "df['Update Actions'] = df['Update Actions'].str.replace(phrase_to_remove, '')\n",
    "\n",
    "# Save the modified DataFrame back to Excel\n",
    "output_file = 'final_report.xlsx'\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(\"Data cleaned and saved successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
